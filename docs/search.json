[
  {
    "objectID": "coursework.html",
    "href": "coursework.html",
    "title": "Coursework",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nStatistical Genetics\n\n\n\nGenetics\n\n\nBiostatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning in Python with Sci-Kit Learn\n\n\n\nPython\n\n\nML\n\n\nPandas\n\n\nSKLearn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Prep and Exploratory Data Analysis-Maven Analytics\n\n\n\nPython\n\n\nPandas\n\n\nSeaborn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Tidy Tuesday",
    "section": "",
    "text": "Tidy Tuesday 2026-01-12\n\n\n\n\n\n\n\n\nJan 12, 2026\n\n\nTing Huang\n\n\n\n\n\n\n\n\n\n\n\n\nTypes of Car Models Produced by Country\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\nTing Huang\n\n\n\n\n\n\n\n\n\n\n\n\nCommon Reasons for Terminating Himalayan Climbs (2020-2024)\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Rmd/Content Summary 2.html",
    "href": "Rmd/Content Summary 2.html",
    "title": "Multiple Testing and PCA in the Context of Statistical Genetics",
    "section": "",
    "text": "Often when we work with genetic data, our goal when running models is to find the SNPs with the smallest p-values. This allows us to find SNPs of significance that may give us insight into which SNPs affect certain traits. However, there are many factors that can make this process much more complicated and need to be accounted for. In this article, we will go over two of these factors: conducting multiple hypothesis tests and genetic ancestry."
  },
  {
    "objectID": "Rmd/Content Summary 2.html#introduction",
    "href": "Rmd/Content Summary 2.html#introduction",
    "title": "Multiple Testing and PCA in the Context of Statistical Genetics",
    "section": "",
    "text": "Often when we work with genetic data, our goal when running models is to find the SNPs with the smallest p-values. This allows us to find SNPs of significance that may give us insight into which SNPs affect certain traits. However, there are many factors that can make this process much more complicated and need to be accounted for. In this article, we will go over two of these factors: conducting multiple hypothesis tests and genetic ancestry."
  },
  {
    "objectID": "Rmd/Content Summary 2.html#hypothesis-testing",
    "href": "Rmd/Content Summary 2.html#hypothesis-testing",
    "title": "Multiple Testing and PCA in the Context of Statistical Genetics",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nWe determine whether a SNP is small enough to be statistically significant through a process called hypothesis testing. The goal of hypothesis testing is to help us decide between two conflicting theories, the null hypothesis and the alternative hypothesis. In general, the null hypothesis suggests that there is no relationship between the variable being tested and the output. The alternative suggests that there is a relationship between the variable being tested and the output. In the case of genetic data, our null hypothesis is that there is no relationship between the trait we are interested in and the current SNP being tested. The alternative hypothesis states that there is a relationship between the trait and the SNP. But how do we decide which one is true?\nWe use p-values to determine how significant the relationship between the explanatory variable and the output is. Larger p-values indicate less significant or no relationship and smaller p-values indicate more significance. However, what it large and small is totally relative. So we must choose a threshold for our p-values to be larger or smaller than to determine significance. This chosen value is arbitrary but commonly a threshold of 0.05 is used to test whether a variable is significant or not. Using a threshold of 0.05, any value above 0.05 would not be considered significant and any value below would be considered significant. As values get closer 0.05, the interpretation of what is significant and what isn’t can become a little bit more complex. For example, is there is really a difference in significance for variables with p-values between 0.049 and 0.051? Arguably not and some researchers use terms like ‘borderline significant’ to refer to p-values that are close to the threshold though the concept of what is close or not is also fairly subjective. However, for now, we will follow the general rule that any p-value larger than the threshold is not significant and any p-value smaller than the threshold is significant.\n\nP-Values\nWhat is a p-value? When conducting hypothesis tests, there are 4 types of results we can get. Firstly, we can fail to reject the null hypothesis when the null is true. We can also reject the null hypothesis when the alternative is true. Both of these are good results as we decided on the course of action that was aligned with the truth. However, it should be noted, that when working with real data, we will most likely not know the truth. There are also two types of errors that can occur. A Type I error occurs when we reject the null hypothesis and say that the relationship between a variable and the output is significant when actually, the null hypothesis is true and there is no relationship. A Type II error occurs when we fail to reject the null hypothesis and say that the relationship is not significant when there is actually a significant relationship. A p-value is the probability of obtaining results at least as extreme as the observed ones. To determine which p-values we consider significant, we choose a threshold oftentimes labeled \\(\\alpha\\). This is also equal to probability of making a Type I error. So with a p-value threshold of 0.05, we are allowing there to be a 5% chance that we make a Type I error. Ideally, we would like to minimize error as much as possible. However, aggressively minimizing one type of error can lead to increases in the probability of the other type of error occurring so a balance must be found depending on how concerned we are with each error type."
  },
  {
    "objectID": "Rmd/Content Summary 2.html#multiple-testing",
    "href": "Rmd/Content Summary 2.html#multiple-testing",
    "title": "Multiple Testing and PCA in the Context of Statistical Genetics",
    "section": "Multiple Testing",
    "text": "Multiple Testing\nWhen we work with genetic data, we will have to conduct a hypothesis test for every SNP. For each independent hypothesis test, there will be a 5% chance that we make a Type I error. This begs the question, what is the probability of making a Type I error across all the tests? If we conducted 10 independent tests at a threshold of 0.05 then we can determine the probability of making at least 1 Type I error across all the tests from the following equation:\n\\(P(\\text{at least 1 T1E}) = 1 - [1 - 0.05]^{10} \\cong 0.4\\)\nMore generally, we can view this equation like the following, where \\(\\alpha\\) is our desired Type I error rate and \\(m\\) is the number of hypothesis tests we are conducting:\n\\(P(\\text{at least 1 T1E}) = 1 - [1 - \\alpha]^m\\)\nTo get the probability of getting at least 1 Type I error we can start by getting the probability of not getting a Type I error which is \\(1-0.05\\). Since we did 10 tests, we multiply the probability of not getting a Type I error for each test with each other, which was accomplished with the exponent of 10 since the probability is the same for all 10 tests, to get the probability of not getting a Type I error across all tests. We then take 1 - P(not getting a Type I error across all tests) in order to get the probability of getting at least 1 Type I error across all tests. At a threshold of 0.05 for each independent hypothesis test, our probability of getting at least 1 Type I error across all tests, otherwise known as family-wise error rate (FWER), is .4 which means that there is a 40% chance that at least 1 Type I error will occur. This is much higher than our desired chance of 5%. This signifies that we need to make our p-value threshold smaller for the individual tests so that our family-wise error rate is reasonable. But how small should we make them?\n\nBonferroni Correction\nThere are several ways to approach this question but one very common method is known as the Bonferroni Correction. The Bonferroni Correction states that we can calculate the significance threshold of the individual hypothesis tests by dividing our desired family-wise error rate by the number of hypothesis tests. To make that a little bit more visual, if \\(\\alpha\\) is our desired FWER and \\(m\\) is the number of hypothesis tests we are conducting then our new significance threshold is:\n\\(\\frac{\\alpha}{m}\\)\nSo if we desired a FWER of 0.05 and we were conducting 10 hypothesis tests, our new significance threshold under the Bonferroni correction would be:\n\\(\\frac{0.05}{10} = .005\\)\nWe can prove that this method works mathematically using Boole’s Inequality. As stated above, FWER is the probability of making at least one Type I error. Therefore, FWER is the probability of rejecting at least 1 true null hypothesis. We can write that probability like so:\n\\(P(\\bigcup_{i=1}^{m_0}(p_i \\leq \\frac{\\alpha}{m}))\\)\nwhere \\(m\\) is the number of hypotheses being tested, \\(m_0\\) is the number of true null hypotheses, \\(\\alpha\\) is our desired significance threshold, and \\(p_i\\) is the associated p-value for each hypothesis test. The large union bound \\(\\bigcup\\) is used to show the union between multiple events, in this case, hypothesis tests from 1 to \\(m_0\\). Boole’s inequality states that\n\\(P(\\bigcup_{i=1}^{n} A_i \\leq \\sum_{i=1}^n P(A_i)\\).\nWe can prove this statement through induction. Since\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\),\nlet’s say\n\\(P(\\bigcup_{i=1}^{n+1} A_i) = P(\\bigcup_{i=1}^n A_i) + P(A_{n+1}) - P(\\bigcup_{i=1}^n A_i \\cap A_{n+1})\\)\nSince all probabilities must be greater than equal to 0, we know that\n\\(P(\\bigcup_{i=1}^{n+1} A_i) \\leq P(\\bigcup_{i=1}^n A_i) + P(A_{n+1})\\)\nsince we are no longer taking away a number that must be 0 or positive.\n\\(P(\\bigcup_{i=1}^{n+1} A_i) \\leq \\sum_{i=1}^n P(A_i) + P(A_{n+1}) = \\sum_{i=1}^{n+1} P(A_i)\\)\nNow that we know Boole’s Inequality is true, let’s use it to prove the Bonferroni Correction.\n\\(P(\\bigcup_{i=1}^{m_0}(p_i \\leq \\frac{\\alpha}{m})) \\leq \\sum_{i=1}^{m_0} (p_i \\leq \\frac{\\alpha}{m})) = m_0\\frac{\\alpha}{m}\\)\nSince we will not know how many true null hypotheses there are, we assume that the null hypothesis is always true. This means we can write the above statement as\n\\(P(\\bigcup_{i=1}^{m}(p_i \\leq \\frac{\\alpha}{m})) \\leq \\sum_{i=1}^{m} (p_i \\leq \\frac{\\alpha}{m})) = m\\frac{\\alpha}{m} = \\alpha\\).\nHowever, this method only works well when conducting multiple independent hypothesis tests. When data is correlated, as it often is in genetic data, then the Bonferoni Correction can be overly conservative in its calculation of the significance threshold. In other words, the significance threshold is smaller than it needs to be which, as mentioned before, reduces the chance even further of having a Type I error but increases the chance of having Type II errors. So instead, we need a method that only divides our desired family-wise error rate by the number of independent hypothesis tests being conducted.\n\n\nSimulation Based Approach\nAnother approach to multiple testing is a simulation based approach. This approach involves simulating a null trait which is a trait that is not associated with any of the SNPs. Then we run GWAS to test the association between the trait the SNPs and record the smallest p-value. This process is repeated many, many times and then look at all the smallest p-values we saved over every replication of the simulation and find the lowest 5th percentile. This value will become our new p-value threshold. This method will work even if tests are correlated unlike the Bonferroni Correction. However, it is computationally expensive.\nIn order to help us understand this method in more depth, we will be running through some code using some data from this Github page specifically from the 1_QC_GWAS file.\nIn order to use the snpStats library, run the following code in the console to install it:\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"snpStats\")\nThe NatParksPalettes package is not necessary but gives us some fun colors to work with.\n\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.3.3\n\nlibrary(snpStats)\n\nLoading required package: survival\n\n\nWarning: package 'survival' was built under R version 4.3.3\n\n\nLoading required package: Matrix\n\nlibrary(NatParksPalettes)\n\nWe need the fam, bim, and bed files to make sure our data looks right. To reduce the computational expense, we will only be looking at the first 100 SNPs.\n\n# update these file paths according to the way you've stored the data on your computer\nfam &lt;- 'HapMap_3_r3_1.fam'\nbim &lt;- 'HapMap_3_r3_1.bim'\nbed &lt;- 'HapMap_3_r3_1.bed'\n\n# then read in the files, using select.snps to select only the first 100\nhapmap &lt;- read.plink(bed, bim, fam, select.snps = 1:100)\n\n# confirm we have 100 SNPs only\nhapmap$genotypes\n\nA SnpMatrix with  165 rows and  100 columns\nRow names:  NA06989 ... NA12865 \nCol names:  rs2185539 ... rs12757754 \n\n\nThere are some monomorphic SNPs in this dataset which we need to remove. Monomorphic SNPs occur when no one in the dataset has the minor allele. These SNPs do not provide any information to us since they are the same for everyone so we remove them.\n\nmaf &lt;- col.summary(hapmap$genotypes)$MAF\nmono &lt;- which(maf == 0)\n\nIn statistical genetics, we refer to correlation between SNPs as linkage disequilibrium (LD) and we can use the ld function in the snpStats package that we can use to calculate the LD for us.\n\n# calculate LD\nhapmap.ld &lt;- ld(hapmap$genotypes[,-mono], depth = 99-length(mono), stats = \"R.squared\", symmetric = TRUE)\n\nWe can use square brackets to define a 5-by-5 matrix to take a look at the first few results of our LD.\n\n# look at the first 5-by-5 elements:\nhapmap.ld[1:5, 1:5]\n\n5 x 5 sparse Matrix of class \"dsCMatrix\"\n             rs11510103   rs3131972   rs3131969   rs1048488   rs12562034\nrs11510103 .            0.005662224 0.007526042 0.005637084 0.0007867133\nrs3131972  0.0056622243 .           0.800991691 1.000000000 0.0229041257\nrs3131969  0.0075260417 0.800991691 .           0.800809717 0.0183460144\nrs1048488  0.0056370836 1.000000000 0.800809717 .           0.0232281120\nrs12562034 0.0007867133 0.022904126 0.018346014 0.023228112 .           \n\n\nIn this matrix, values closer to one indicate higher correlation. For example, rs3131972, rs3131969, and rs1048488 are all highly correlated with each other.\nWe can also use the image function to plot our LD.\n\n# plot LD (grey scale)\nimage(hapmap.ld)\n\n\n\n\n\n\n\n# plot LD (fun color palette)\ncolor.pal &lt;- natparks.pals(\"Acadia\", 10)\nimage(hapmap.ld, lwd = 0, cuts = 9, col.regions = color.pal, colorkey = TRUE)\n\n\n\n\n\n\n\n\nIn this plot, the blocks are colored by how correlated that SNP is with another. Colors associated with 1 indicate that the SNP is completely correlated with another while colors associated with 0 indicate that the SNP is completely independent from other SNPs. However, it should be noted that SNPs can be correlated even if linkage disequilibrium is equal to 0 if the relationship between the SNPs is non-linear.\nBased on the plot above, we can see that there are several SNPs that are highly correlated with each other.\nIn order to make our data easier to work with, we can use the as function with the parameter “numeric” to create a matrix of zeroes, ones, and twos representing the minor allele frequency.\n\nhapmap.geno &lt;- as(hapmap$genotypes, \"numeric\")\n\nOnce again, we must remove the monomorphic SNPs. In order to make sure we have done this correctly, we can check the dimensions of the matrix before and after filtering the monomorphic SNPs out.\n\n# check the dimensions before filtering\ndim(hapmap.geno)\n\n[1] 165 100\n\n# filter genotype matrix to remove monomorphic SNPs\nhapmap.geno &lt;- hapmap.geno[,-mono]\n\n# check the dimensions after filtering\ndim(hapmap.geno)\n\n[1] 165  83\n\n\nIn order to run our simulation, we can define a function that simulates one null trait \\(y\\) and then run GWAS with this null trait. We then find the smallest p-value using the min function and that is recorded as the output of the function.\n\n# write a function to do one simulation replicate\ndo_one_sim_hapmap &lt;- function(){\n  # simulate null trait\n  y &lt;- rnorm(n = 165, mean = 0, sd = 1)\n  # implement GWAS\n  pvals &lt;- c()\n  for(i in 1:83){\n    mod &lt;- lm(y ~ hapmap.geno[,i])\n    pvals[i] &lt;- tidy(mod)$p.value[2]\n  }\n  # check if any pvals are &lt; 0.05\n  min(pvals)\n}\n\nWe then want to run this function several times, in this case 500 times. Since we are generating a random null trait, we set a seed to make sure that each time this code is run, we get the same random null trait. This is very important because if we didn’t do this, our results would different every time we run our code. The number within the set.seed function does not matter as long as it stays the same every time you run your code.\n\n# then repeat many times\nset.seed(494)\nhapreps &lt;- replicate(500, do_one_sim_hapmap())\n\n# then use the quantile function to find the lower 5th percentile\nquantile(hapreps, probs = c(.05))\n\n         5% \n0.001048875 \n\n\nThe quantile function provides us with the percentile of the lower 5% of our simulated p-values. The Bonferroni Correction threshold would be\n\\(0.05/83 = .0006024096\\)\nwhich is much smaller than our simulation approach threshold. This threshold would make it much more difficult to reject the null hypothesis which in turn could result in more type 2 errors."
  },
  {
    "objectID": "Rmd/Content Summary 2.html#genetic-ancestry",
    "href": "Rmd/Content Summary 2.html#genetic-ancestry",
    "title": "Multiple Testing and PCA in the Context of Statistical Genetics",
    "section": "Genetic Ancestry",
    "text": "Genetic Ancestry\nGenetic ancestry refers to the ancestral origin the genetic material we inherit. This differs from genealogical ancestry which is defined more in terms of someone’s family tree rather than their DNA. For example, I might say that I am 1/4 Italian since one of my grandparents was Italian and an immigrant from Italy. However, according to my genetics, I might not actually have 1/4 of my grandparent’s genetic material. This is because as genes are passed on from one generation to another, recombination events (see bottom right of below image) occur that shuffle the genetic material that is passed on. This is why siblings have the same genealogical ancestry but not the same genetic ancestry.\n\nGenetic ancestry is especially important to us when studying methods to analyze our genome since it can be a confounding factor. A confounding factor is a variable, typically unmeasured, which can influence predictors and outcomes. This means it will be correlated to a predictor variable and causally related to the dependent variable. In this case, genetic ancestry can be correlated to genotype since our ancestors influence what genes we get. This will also affect what traits we have. In order to account for genetic ancestry in GWAS, we can add it to our general model.\nHowever, before we can account for genetic ancestry, we need to infer what someone’s genetic ancestry is first. There are several methods to infer genetic ancestry that include both machine learning techniques as well as biologically informed model based approaches. Here, we will focus on one unsupervised machine learning technique: Principal Component Analysis."
  },
  {
    "objectID": "Rmd/Content Summary 2.html#pca",
    "href": "Rmd/Content Summary 2.html#pca",
    "title": "Multiple Testing and PCA in the Context of Statistical Genetics",
    "section": "PCA",
    "text": "PCA\nPrincipal Component Analysis (PCA) is an unsupervised learning machine learning tool that focuses on dimension reduction. It is unsupervised because it does not require outcome values or labels. This means that everything generated by PCA is based purely on the covariates. PCA returns linear combinations of our original variables that are called principal components (PCs). These PCs are generated based on which ones explain the most variability in the data. For example, to generate PC1, all possible linear combinations of the original variables are considered and the one with the most variance is labeled as the first principal component PC1. PC2 will be the linear combination that has the next largest variance given that it is perpendicular or orthogonal to PC1. This process then continues for all other PCs.\n\nThe values that our PCs take are called scores. The coefficients (\\(a\\) in above image) of the linear combination within the PCs are called loadings.\n\nRunning PCA in R\nWe can use R to illustrate and run the PCA process. Let’s start by loading in some packages that we will need.\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nI will be using a simulated ‘toy’ dataset to be demonstrating PCA on. These types of datasets can be really helpful both to confirm to ourselves that the method works and that our code works. This particular dataset has 1000 people in it and 15 SNPs. There are 2 population groups with 500 people in each group. What we also know, because this a simulated dataset, is that the trait depends on the genotype in SNP 3.\n\nToy &lt;- read_csv(\"pca_toy_data.csv\")\n\nRows: 1000 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): population, trait, SNP1, SNP2, SNP3, SNP4, SNP5, SNP6, SNP7, SNP8,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst, let’s take a look at the minor allele frequencies for this dataset.\n\nToy %&gt;% \n  group_by(population) %&gt;% \n  summarise_all(function(x) sum(x)/(2*length(x)))\n\n# A tibble: 2 × 17\n  population trait  SNP1  SNP2  SNP3  SNP4  SNP5  SNP6  SNP7  SNP8  SNP9 SNP10\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1          1 0.268     0     0 0.298 0.291 0.293 0.102 0.107 0.094 0.095 0.097\n2          2 0.997     1     1 0.512 0.506 0.504 0.11  0.085 0.097 0.092 0.111\n# ℹ 5 more variables: SNP11 &lt;dbl&gt;, SNP12 &lt;dbl&gt;, SNP13 &lt;dbl&gt;, SNP14 &lt;dbl&gt;,\n#   SNP15 &lt;dbl&gt;\n\n\nWhat should stand out immediately is the extreme difference in minor allele frequencies in SNPs 1 and 2 where no one has the minor allele in population 1 and everyone has it in population 2. We can also see that the minor allele is more frequent in population 2 in SNPs 3-5.\nSince PCA is an unsupervised learning tool, we need to remove any label variables from our dataset. We can use the select function from the dplyr package and use the ‘-’ symbol within the function to indicate which columns we want to remove.\n\ngeno &lt;- Toy %&gt;% \n  select(-population, -trait)\n\nWe then can use the prcomp function to run PCA. Note that this is not the only way to run PCA in R.\n\npca_out &lt;- prcomp(geno, center = TRUE, scale = TRUE)\n\nIn order to assist with further analysis, we can extract the loadings and scores from our PCs with the following code. The ‘$’ symbol allows us to select one column from the pca_out data frame we created in the previous chunk and the as.data.frame function puts these columns into their own data frame.\n\npca_loadings &lt;- as.data.frame(pca_out$rotation)\n\npca_scores &lt;- as.data.frame(pca_out$x)\n\nThe plot below displays our PC scores for PC1 and PC2. We can see that PC1 divides the data into 2 separate populations.\n\npca_scores %&gt;%\n  as.data.frame() %&gt;% # convert pca_scores into a data frame for plotting\n  ggplot(aes(x = PC1, y = PC2, color = factor(Toy$population))) + # then plot\n  geom_point() + \n  scale_color_viridis_d() +\n  labs(color = \"Population\") + \n  theme_classic()\n\n\n\n\n\n\n\n\nAnother way we can plot scores is a parallel coordinates plot. This plot allows us to visualize all of our PCs at once. In the plot below, we have used color to differentiate between the two populations with each line tracing the scores of each PC for each person in the dataset. While these types of plots can get quite messy, essentially what we are looking for is when the lines for each group separate. The separation indicates that the PC is capturing population membership. In this specific plot, we can see this separation occur in PC1 and PC15 though the latter result should probably be looked at with more scrutiny.\n\n# parallel coordinates plot\npca_scores %&gt;%\n  as.data.frame() %&gt;%\n  mutate(population = as.factor(Toy$population)) %&gt;% \n  ggparcoord(columns = 1:15, groupColumn = 'population', alpha = 0.2) + \n  theme_minimal() + \n  scale_color_brewer(palette = 'Dark2')\n\n\n\n\n\n\n\n\nAnother way to visualize the results of PCA is by plotting the loadings. Here, we can create an index variable to represent the SNPs (which are now the rows) so that we can use a point plot as our visualization. This way we can easily see what the values of the loadings are for each SNP. We create the index variable by adding it to the pca_loadings dataframe with the mutate function from dplyr and setting that variable equal to a sequence created from the seq_len function which is as long as the number of rows in pca_loadings which is obtained through the nrow function.\n\npca_loadings %&gt;%\n  as.data.frame() %&gt;%\n  mutate(index = seq_len(nrow(pca_loadings))) %&gt;%\n  ggplot(aes(x = index, y = PC1)) + \n  geom_point() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the plot above, we can see that the first two SNPs have the highest loadings and SNPs 6-15 have loadings that are almost 0. SNPs 3-5 have loadings smaller than the first 2 SNPs but still fairly bigger than the rest of the SNPs. This makes sense since these are the SNPs with major differences in minor allele frequency between the two populations.\n\np1 &lt;- pca_loadings %&gt;%\n  as.data.frame() %&gt;%\n  mutate(index = seq_len(nrow(pca_loadings))) %&gt;%\n  ggplot(aes(x = index, y = PC2)) + \n  geom_point() + \n  theme_minimal()\n\np2 &lt;- pca_loadings %&gt;%\n  as.data.frame() %&gt;%\n  mutate(index = seq_len(nrow(pca_loadings))) %&gt;%\n  ggplot(aes(x = index, y = PC3)) + \n  geom_point() + \n  theme_minimal()\n\np3 &lt;- pca_loadings %&gt;%\n  as.data.frame() %&gt;%\n  mutate(index = seq_len(nrow(pca_loadings))) %&gt;%\n  ggplot(aes(x = index, y = PC4)) + \n  geom_point() + \n  theme_minimal()\n\ngrid.arrange(p1, p2, p3, ncol = 1)\n\n\n\n\n\n\n\n\nIn these plots we really don’t see as much of a pattern in terms of which SNPs have more weight than others.\nWe can also view the PCs through the amount of variance they explain. While we know that PC1 explains the most variance overall, we don’t know exactly how much it explains in relation with the other PCs. We can view this by calculating the proportion of variance each PC explains and plotting the results on what it is known as a scree plot. First, we can obtain the variance for each PC by squaring the standard deviation since each PC is a linear combination.\n\n# extract variance of each PC\npca_var &lt;- (pca_out$sdev)^2\n\nTo create the proportion, we need to sum over the variation of each PC to get the total PC and then divide the individual variance of each PC by the total variation.\n\n# calculate proportion of variance explained\ntotal_var &lt;- sum(pca_var)\npve &lt;- pca_var/total_var\n\nThis gives us a percentage that we can plot against the number of each PC which another index variable will help us do.\n\n# scree plot\npve %&gt;%\n  as.data.frame() %&gt;%\n  mutate(index = seq_len(length(pca_var))) %&gt;%\n  ggplot(aes(x = index, y = pve)) + \n  geom_point() + \n  geom_line() + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot illustrates that PC1 explains over 15% of the variance, PCs 2-14 explain approximately between 5% and 7% and PC15 explains essentially 0% of the variance, confirming our skepticism of the results from the parallel coordinates plot from earlier. The way in which scree plots can help us is by giving us an idea of how many PCs we want to include in a GWAS. We can do this through the “elbow” method which is when we look for the point at which the plot begins to flatten (the elbow) and use all PCs prior to that point. In this case, the elbow is at PC2 and so we would only want to include PC1 in our GWAS.\n\n\nAdjusting for PCs in GWAS\nTo illustrate the effect of adjusting for PCs in GWAS, we are going to compare an unadjusted GWAS and an adjusted GWAS. Most of this code should be understandable if you are familiar with running GWAS in R.\n\n# empty vector to store p-values\npvals &lt;- c()\n\n# loop through the 15 SNPs\nfor(i in 1:15){\n  dat &lt;- Toy %&gt;% select(trait, paste0('SNP',i)) # pull out just the trait and SNP of interest\n  mod &lt;- lm(trait ~ ., data = dat) # regress trait on everything else (.), which is just SNP i in this case\n  pvals[i] &lt;- summary(mod)$coef[2,4] # pull out the p-value for the slope\n}\n\n# plot -log10 pvalues\ndata.frame(p = pvals, SNP = 1:15) %&gt;%\n  ggplot(aes(y = -log10(p), x = SNP)) + \n  geom_point() + \n  theme_minimal() + \n  ggtitle('Unadjusted Analysis')\n\n\n\n\n\n\n\n\nFrom this plot, we can see that SNP 3 has the smallest p-value and SNPs 1 and 2 have relatively small p-values compared to the rest of the SNPs but not nearly as small as SNP 3. Now, let’s look at the affect of adjusting for PC1 in GWAS.\n\n# empty vector to store p-values\npvals &lt;- c()\n\n# loop through the 15 SNPs\nfor(i in 1:15){\n  dat &lt;- Toy %&gt;% \n    select(trait, paste0('SNP',i)) %&gt;% # pull out just the trait and SNP of interest\n    mutate(PC1 = pca_scores[,1]) # add the scores for PC1\n  mod &lt;- lm(trait ~ ., data = dat) # regress trait on everything else (.), which is SNP i and PC1\n  pvals[i] &lt;- summary(mod)$coef[2,4] # pull out the p-value for the slope\n}\n\n# plot -log10 pvalues\ndata.frame(p = pvals, SNP = 1:15) %&gt;%\n  ggplot(aes(y = -log10(p), x = SNP)) + \n  geom_point() + \n  theme_minimal() + \n  ggtitle('Adjusted Analysis')\n\n\n\n\n\n\n\n\nWe can see that this model is clearly better, only highlighting SNP 3 as having a low p-value. Since we know that SNP 3 has the genotype that the trait is based on, we can tell that this model is better because it accounts for the strong association with the trait displayed by SNPs 1 and 2 because of population. By accounting for population with PC1, SNPs 1 and 2, correctly do not have a strong association with the trait because in reality, only SNP 3 should have a strong association with the trait."
  },
  {
    "objectID": "Rmd/Project_Final_Draft.html",
    "href": "Rmd/Project_Final_Draft.html",
    "title": "Project Final Draft",
    "section": "",
    "text": "library(survival)\n\nWarning: package 'survival' was built under R version 4.3.3\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n\n# define column names for easy indexing\nindex_names = c('unit_nr', 'time_cycles')\nsetting_names = c('altitude', 'mach', 'TRA')\nsensor_names = c()\nfor(i in 1:21) {\n  name &lt;- paste0(\"s_\", i)\n  sensor_names[i] = name\n}\ncol_names = c(index_names,setting_names,sensor_names)\n\n\ndata1 &lt;- read.table(file = \"train_FD001.txt\", col.names = col_names)\n\n\nsummary(data1)\n\n    unit_nr        time_cycles       altitude              mach           \n Min.   :  1.00   Min.   :  1.0   Min.   :-8.70e-03   Min.   :-6.000e-04  \n 1st Qu.: 26.00   1st Qu.: 52.0   1st Qu.:-1.50e-03   1st Qu.:-2.000e-04  \n Median : 52.00   Median :104.0   Median : 0.00e+00   Median : 0.000e+00  \n Mean   : 51.51   Mean   :108.8   Mean   :-8.87e-06   Mean   : 2.351e-06  \n 3rd Qu.: 77.00   3rd Qu.:156.0   3rd Qu.: 1.50e-03   3rd Qu.: 3.000e-04  \n Max.   :100.00   Max.   :362.0   Max.   : 8.70e-03   Max.   : 6.000e-04  \n      TRA           s_1             s_2             s_3            s_4      \n Min.   :100   Min.   :518.7   Min.   :641.2   Min.   :1571   Min.   :1382  \n 1st Qu.:100   1st Qu.:518.7   1st Qu.:642.3   1st Qu.:1586   1st Qu.:1402  \n Median :100   Median :518.7   Median :642.6   Median :1590   Median :1408  \n Mean   :100   Mean   :518.7   Mean   :642.7   Mean   :1591   Mean   :1409  \n 3rd Qu.:100   3rd Qu.:518.7   3rd Qu.:643.0   3rd Qu.:1594   3rd Qu.:1415  \n Max.   :100   Max.   :518.7   Max.   :644.5   Max.   :1617   Max.   :1441  \n      s_5             s_6             s_7             s_8            s_9      \n Min.   :14.62   Min.   :21.60   Min.   :549.9   Min.   :2388   Min.   :9022  \n 1st Qu.:14.62   1st Qu.:21.61   1st Qu.:552.8   1st Qu.:2388   1st Qu.:9053  \n Median :14.62   Median :21.61   Median :553.4   Median :2388   Median :9061  \n Mean   :14.62   Mean   :21.61   Mean   :553.4   Mean   :2388   Mean   :9065  \n 3rd Qu.:14.62   3rd Qu.:21.61   3rd Qu.:554.0   3rd Qu.:2388   3rd Qu.:9069  \n Max.   :14.62   Max.   :21.61   Max.   :556.1   Max.   :2389   Max.   :9245  \n      s_10          s_11            s_12            s_13           s_14     \n Min.   :1.3   Min.   :46.85   Min.   :518.7   Min.   :2388   Min.   :8100  \n 1st Qu.:1.3   1st Qu.:47.35   1st Qu.:521.0   1st Qu.:2388   1st Qu.:8133  \n Median :1.3   Median :47.51   Median :521.5   Median :2388   Median :8141  \n Mean   :1.3   Mean   :47.54   Mean   :521.4   Mean   :2388   Mean   :8144  \n 3rd Qu.:1.3   3rd Qu.:47.70   3rd Qu.:522.0   3rd Qu.:2388   3rd Qu.:8148  \n Max.   :1.3   Max.   :48.53   Max.   :523.4   Max.   :2389   Max.   :8294  \n      s_15            s_16           s_17            s_18           s_19    \n Min.   :8.325   Min.   :0.03   Min.   :388.0   Min.   :2388   Min.   :100  \n 1st Qu.:8.415   1st Qu.:0.03   1st Qu.:392.0   1st Qu.:2388   1st Qu.:100  \n Median :8.439   Median :0.03   Median :393.0   Median :2388   Median :100  \n Mean   :8.442   Mean   :0.03   Mean   :393.2   Mean   :2388   Mean   :100  \n 3rd Qu.:8.466   3rd Qu.:0.03   3rd Qu.:394.0   3rd Qu.:2388   3rd Qu.:100  \n Max.   :8.585   Max.   :0.03   Max.   :400.0   Max.   :2388   Max.   :100  \n      s_20            s_21      \n Min.   :38.14   Min.   :22.89  \n 1st Qu.:38.70   1st Qu.:23.22  \n Median :38.83   Median :23.30  \n Mean   :38.82   Mean   :23.29  \n 3rd Qu.:38.95   3rd Qu.:23.37  \n Max.   :39.43   Max.   :23.62  \n\n\n\ndata1 &lt;- data1%&gt;% \n  group_by(unit_nr) %&gt;% \n  mutate(max_cycle = max(time_cycles)) %&gt;% \n  mutate(RUL = max_cycle-time_cycles)\n\n\ndata1 %&gt;% \n  group_by(unit_nr) %&gt;% \n  summarise(r = max(RUL)) %&gt;% \n  ggplot() +\n  geom_histogram(aes(x = r), bins = 15) +\n  labs(title = \"Number of Failures of Turbofan Jet Engines over Time\", x = \"Time Cycles\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nt1 = apply(data1[,c('s_1','s_2','s_3','s_4','s_5','s_6','s_7','s_8','s_9','s_10','s_11','s_12','s_13','s_14','s_15','s_16','s_17','s_18','s_19','s_20','s_21')],2,sd)\n\n\ntab1 = matrix(t1, ncol = 1, byrow = TRUE)\ncolnames(tab1) = \"SD\"\nrownames(tab1) = c('s_1','s_2','s_3','s_4','s_5','s_6','s_7','s_8','s_9','s_10','s_11','s_12','s_13','s_14','s_15','s_16','s_17','s_18','s_19','s_20','s_21')\ntab1 = as.table(tab1)\n\n\ntab1\n\n               SD\ns_1   0.000000000\ns_2   0.500053270\ns_3   6.131149520\ns_4   9.000604781\ns_5   0.000000000\ns_6   0.001388985\ns_7   0.885092258\ns_8   0.070985479\ns_9  22.082879525\ns_10  0.000000000\ns_11  0.267087399\ns_12  0.737553392\ns_13  0.071918916\ns_14 19.076175976\ns_15  0.037505038\ns_16  0.000000000\ns_17  1.548763025\ns_18  0.000000000\ns_19  0.000000000\ns_20  0.180746428\ns_21  0.108250875\n\n\n\ndata1 &lt;- data1 %&gt;% \n  mutate(breakdown = ifelse(time_cycles == max_cycle,\n                            yes = 1,\n                            no = 0))\n\n\ndata1 &lt;- data1 %&gt;% \n  mutate(start = time_cycles-1)\n\n\ndata_n &lt;- data1 %&gt;% \n  filter(breakdown == 1)\n\n\nKM1 = survfit(Surv(time_cycles)~1, data = data_n)\n\n\nplot(KM1, ylab = \"Survival Proportion\", xlab = \"Time Cycles\")\n\n\n\n\n\n\n\n\n\nTime-Varying Stuff\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_9, data = data1)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_9, data = data1)\n\n        coef exp(coef) se(coef)     z        p\ns_9 0.011925  1.011996 0.002384 5.002 5.67e-07\n\nLikelihood ratio test=22.01  on 1 df, p=2.707e-06\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_9 + s_9:time_cycles, data = data1)\n\nWarning in coxph(Surv(start, time_cycles, breakdown) ~ s_9 + s_9:time_cycles, :\na variable appears on both the left and right sides of the formula\n\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_9 + s_9:time_cycles, \n    data = data1)\n\n                      coef  exp(coef)   se(coef)      z      p\ns_9             -2.026e-02  9.799e-01  1.758e-02 -1.152 0.2492\ns_9:time_cycles  1.554e-04  1.000e+00  8.413e-05  1.847 0.0647\n\nLikelihood ratio test=25.7  on 2 df, p=2.632e-06\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_14, data = data1)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_14, data = data1)\n\n         coef exp(coef) se(coef)     z        p\ns_14 0.011050  1.011112 0.002813 3.928 8.57e-05\n\nLikelihood ratio test=13.84  on 1 df, p=0.0001992\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_14 + s_14:time_cycles, data = data1)\n\nWarning in coxph(Surv(start, time_cycles, breakdown) ~ s_14 + s_14:time_cycles,\n: a variable appears on both the left and right sides of the formula\n\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_14 + \n    s_14:time_cycles, data = data1)\n\n                       coef  exp(coef)   se(coef)      z      p\ns_14             -3.799e-02  9.627e-01  2.068e-02 -1.837 0.0663\ns_14:time_cycles  2.370e-04  1.000e+00  9.899e-05  2.394 0.0167\n\nLikelihood ratio test=20.14  on 2 df, p=4.228e-05\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_4, data = data1)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_4, data = data1)\n\n      coef exp(coef) se(coef)     z      p\ns_4 0.3188    1.3755   0.0241 13.23 &lt;2e-16\n\nLikelihood ratio test=328.9  on 1 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_4 + s_4:time_cycles, data = data1)\n\nWarning in coxph(Surv(start, time_cycles, breakdown) ~ s_4 + s_4:time_cycles, :\na variable appears on both the left and right sides of the formula\n\n\nWarning in agreg.fit(X, Y, istrat, offset, init, control, weights = weights, :\nRan out of iterations and did not converge\n\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_4 + s_4:time_cycles, \n    data = data1)\n\n                      coef  exp(coef)   se(coef)      z        p\ns_4              0.4838592  1.6223231  0.1216414  3.978 6.96e-05\ns_4:time_cycles -0.0008467  0.9991537  0.0005971 -1.418    0.156\n\nLikelihood ratio test=330.8  on 2 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_3, data = data1)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_3, data = data1)\n\n       coef exp(coef) se(coef)     z      p\ns_3 0.23189   1.26099  0.02078 11.16 &lt;2e-16\n\nLikelihood ratio test=146.6  on 1 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_3 + s_3:time_cycles, data = data1)\n\nWarning in coxph(Surv(start, time_cycles, breakdown) ~ s_3 + s_3:time_cycles, :\na variable appears on both the left and right sides of the formula\n\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_3 + s_3:time_cycles, \n    data = data1)\n\n                      coef  exp(coef)   se(coef)      z        p\ns_3              0.4215603  1.5243381  0.1093176  3.856 0.000115\ns_3:time_cycles -0.0009639  0.9990366  0.0005394 -1.787 0.073958\n\nLikelihood ratio test=149.6  on 2 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_11, data = data1)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_11, data = data1)\n\n          coef exp(coef)  se(coef)     z      p\ns_11 9.726e+00 1.674e+04 7.478e-01 13.01 &lt;2e-16\n\nLikelihood ratio test=298.1  on 1 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_11 + s_11:time_cycles, data = data1)\n\nWarning in coxph(Surv(start, time_cycles, breakdown) ~ s_11 + s_11:time_cycles,\n: a variable appears on both the left and right sides of the formula\n\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_11 + \n    s_11:time_cycles, data = data1)\n\n                       coef  exp(coef)   se(coef)     z        p\ns_11              1.741e+01  3.656e+07  3.827e+00  4.55 5.36e-06\ns_11:time_cycles -3.881e-02  9.619e-01  1.840e-02 -2.11   0.0349\n\nLikelihood ratio test=302.2  on 2 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_17 + s_11 + s_4 + s_9 + s_3, data = data1)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_17 + \n    s_11 + s_4 + s_9 + s_3, data = data1)\n\n          coef exp(coef)  se(coef)     z        p\ns_17 5.157e-01 1.675e+00 1.264e-01 4.082 4.47e-05\ns_11 6.036e+00 4.182e+02 9.937e-01 6.074 1.25e-09\ns_4  2.257e-01 1.253e+00 2.970e-02 7.599 2.98e-14\ns_9  5.505e-04 1.001e+00 2.825e-03 0.195   0.8455\ns_3  4.884e-02 1.050e+00 2.867e-02 1.703   0.0885\n\nLikelihood ratio test=412.8  on 5 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_3 + s_4 + s_9 + s_11, data = data1)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_3 + s_4 + \n    s_9 + s_11, data = data1)\n\n          coef exp(coef)  se(coef)     z        p\ns_3  5.713e-02 1.059e+00 2.847e-02 2.006   0.0448\ns_4  2.302e-01 1.259e+00 2.874e-02 8.011 1.14e-15\ns_9  5.227e-03 1.005e+00 2.588e-03 2.020   0.0434\ns_11 6.415e+00 6.109e+02 9.524e-01 6.735 1.64e-11\n\nLikelihood ratio test=395.3  on 4 df, p=&lt; 2.2e-16\nn= 20631, number of events= 100 \n\n\n\ntab2 = matrix(c(5.713e-02, 1.25855, 0.0448, 2.302e-01, 1.259e+00, 1.14e-15, 5.227e-03, 1.005e+00, 0.0434, 6.415e+00, 6.109e+02, 1.64e-11), ncol = 3, byrow = TRUE)\ncolnames(tab2) = c(\"coef\", \"exp(coef)\", \"p-value\")\nrownames(tab2) = c('Sensor 3','Sensor 4','Sensor 9','Sensor 11')\ntab2 = as.table(tab2)\ntab2\n\n                 coef   exp(coef)     p-value\nSensor 3  5.71300e-02 1.25855e+00 4.48000e-02\nSensor 4  2.30200e-01 1.25900e+00 1.14000e-15\nSensor 9  5.22700e-03 1.00500e+00 4.34000e-02\nSensor 11 6.41500e+00 6.10900e+02 1.64000e-11\n\n\n\ntab3 = matrix(c(4.884e-02, 1.050e+00, 0.0885, 2.257e-01, 1.253e+00, 2.98e-14, 5.505e-04, 1.001e+00, 0.8455, 6.036e+00, 4.182e+02, 1.25e-09, 5.157e-01, 1.675e+00, 4.47e-05), ncol = 3, byrow = TRUE)\ncolnames(tab3) = c(\"coef\", \"exp(coef)\", \"p-value\")\nrownames(tab3) = c('Sensor 3','Sensor 4','Sensor 9','Sensor 11', 'Sensor 17')\ntab3 = as.table(tab3)\ntab3\n\n               coef exp(coef)   p-value\nSensor 3  4.884e-02 1.050e+00 8.850e-02\nSensor 4  2.257e-01 1.253e+00 2.980e-14\nSensor 9  5.505e-04 1.001e+00 8.455e-01\nSensor 11 6.036e+00 4.182e+02 1.250e-09\nSensor 17 5.157e-01 1.675e+00 4.470e-05\n\n\n\ndata1a = data1\n\n\ndata1a$s_9lag = c(NA, data1a$s_9[-nrow(data1a)])\ndata1a$s_9lag[which(!duplicated(data1a$unit_nr))] = NA\n\n\ndata1a\n\n# A tibble: 20,631 × 31\n# Groups:   unit_nr [100]\n   unit_nr time_cycles altitude   mach   TRA   s_1   s_2   s_3   s_4   s_5   s_6\n     &lt;int&gt;       &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       1           1  -0.0007  -4e-4   100  519.  642. 1590. 1401.  14.6  21.6\n 2       1           2   0.0019  -3e-4   100  519.  642. 1592. 1403.  14.6  21.6\n 3       1           3  -0.0043   3e-4   100  519.  642. 1588. 1404.  14.6  21.6\n 4       1           4   0.0007   0      100  519.  642. 1583. 1402.  14.6  21.6\n 5       1           5  -0.0019  -2e-4   100  519.  642. 1583. 1406.  14.6  21.6\n 6       1           6  -0.0043  -1e-4   100  519.  642. 1584. 1398.  14.6  21.6\n 7       1           7   0.001    1e-4   100  519.  642. 1592. 1398.  14.6  21.6\n 8       1           8  -0.0034   3e-4   100  519.  643. 1583. 1401.  14.6  21.6\n 9       1           9   0.0008   1e-4   100  519.  642. 1591. 1395.  14.6  21.6\n10       1          10  -0.0033   1e-4   100  519.  642. 1591. 1400.  14.6  21.6\n# ℹ 20,621 more rows\n# ℹ 20 more variables: s_7 &lt;dbl&gt;, s_8 &lt;dbl&gt;, s_9 &lt;dbl&gt;, s_10 &lt;dbl&gt;, s_11 &lt;dbl&gt;,\n#   s_12 &lt;dbl&gt;, s_13 &lt;dbl&gt;, s_14 &lt;dbl&gt;, s_15 &lt;dbl&gt;, s_16 &lt;dbl&gt;, s_17 &lt;int&gt;,\n#   s_18 &lt;int&gt;, s_19 &lt;dbl&gt;, s_20 &lt;dbl&gt;, s_21 &lt;dbl&gt;, max_cycle &lt;int&gt;, RUL &lt;int&gt;,\n#   breakdown &lt;dbl&gt;, start &lt;dbl&gt;, s_9lag &lt;dbl&gt;\n\n\n\ndata1a$s_11lag = c(NA, data1a$s_11[-nrow(data1a)])\ndata1a$s_11lag[which(!duplicated(data1a$unit_nr))] = NA\n\n\ndata1a$s_4lag = c(NA, data1a$s_4[-nrow(data1a)])\ndata1a$s_4lag[which(!duplicated(data1a$unit_nr))] = NA\n\n\ndata1a$s_3lag = c(NA, data1a$s_3[-nrow(data1a)])\ndata1a$s_3lag[which(!duplicated(data1a$unit_nr))] = NA\n\n\ndata1a$s_17lag = c(NA, data1a$s_17[-nrow(data1a)])\ndata1a$s_17lag[which(!duplicated(data1a$unit_nr))] = NA\n\n\ncoxph(Surv(start, time_cycles, breakdown) ~ s_11lag + s_4lag + s_3lag + s_9lag + s_17lag, data = data1a)\n\nCall:\ncoxph(formula = Surv(start, time_cycles, breakdown) ~ s_11lag + \n    s_4lag + s_3lag + s_9lag + s_17lag, data = data1a)\n\n             coef exp(coef)  se(coef)     z        p\ns_11lag 8.318e+00 4.097e+03 1.064e+00 7.817 5.41e-15\ns_4lag  1.454e-01 1.156e+00 2.884e-02 5.040 4.64e-07\ns_3lag  7.274e-02 1.075e+00 2.988e-02 2.435   0.0149\ns_9lag  3.127e-03 1.003e+00 2.713e-03 1.153   0.2491\ns_17lag 2.633e-01 1.301e+00 1.233e-01 2.136   0.0327\n\nLikelihood ratio test=377.6  on 5 df, p=&lt; 2.2e-16\nn= 20531, number of events= 100 \n   (100 observations deleted due to missingness)\n\n\n\ntab4 = matrix(c(7.404e-02, 1.077e+00, 0.0125, 1.562e-01, 1.169e+00, 3.22e-08, 4.999e-03, 1.005e+00, 0.0533, 8.460e+00, 4.722e+03, 2.76e-16), ncol = 3, byrow = TRUE)\ncolnames(tab4) = c(\"coef\", \"exp(coef)\", \"p-value\")\nrownames(tab4) = c('Sensor 3','Sensor 4','Sensor 9','Sensor 11')\ntab4 = as.table(tab4)\ntab4\n\n               coef exp(coef)   p-value\nSensor 3  7.404e-02 1.077e+00 1.250e-02\nSensor 4  1.562e-01 1.169e+00 3.220e-08\nSensor 9  4.999e-03 1.005e+00 5.330e-02\nSensor 11 8.460e+00 4.722e+03 2.760e-16\n\n\n\ntab5 = matrix(c(7.274e-02, 1.075e+00, 0.0149, 1.454e-01, 1.156e+00, 4.64e-07, 3.127e-03, 1.003e+00, 0.2491, 8.318e+00, 4.097e+03, 5.41e-15, 2.633e-01, 1.301e+00, 0.0327), ncol = 3, byrow = TRUE)\ncolnames(tab5) = c(\"coef\", \"exp(coef)\", \"p-value\")\nrownames(tab5) = c('Sensor 3','Sensor 4','Sensor 9','Sensor 11', 'Sensor 17')\ntab5 = as.table(tab5)\ntab5\n\n               coef exp(coef)   p-value\nSensor 3  7.274e-02 1.075e+00 1.490e-02\nSensor 4  1.454e-01 1.156e+00 4.640e-07\nSensor 9  3.127e-03 1.003e+00 2.491e-01\nSensor 11 8.318e+00 4.097e+03 5.410e-15\nSensor 17 2.633e-01 1.301e+00 3.270e-02\n\n\n\ndata1 %&gt;% \n  ggplot(aes(x = time_cycles)) + \n  geom_point(aes(y = s_3)) +\n  xlim(c(0,375)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata1 %&gt;% \n  ggplot(aes(x = time_cycles)) + \n  geom_point(aes(y = s_4)) +\n  xlim(c(0,375)) +\n  theme_minimal() + \n  labs(title = \"Sensor 4 Readings over Time\", x = \"Time Cycles\", y = \"Sensor 4\")\n\n\n\n\n\n\n\n\n\ndata1 %&gt;% \n  ggplot(aes(x = time_cycles)) + \n  geom_point(aes(y = s_9)) +\n  xlim(c(0,375)) +\n  theme_minimal() + \n  labs(title = \"Sensor 9 Readings over Time\", x = \"Time Cycles\", y = \"Sensor 9\")\n\n\n\n\n\n\n\n\n\ndata1 %&gt;% \n  ggplot(aes(x = time_cycles)) + \n  geom_point(aes(y = s_11)) +\n  xlim(c(0,375)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ndata1 %&gt;% \n  ggplot(aes(x = time_cycles)) + \n  geom_point(aes(y = s_17)) +\n  xlim(c(0,375)) +\n  theme_minimal()"
  },
  {
    "objectID": "coursework/Statistical Genetics/StatGen.html",
    "href": "coursework/Statistical Genetics/StatGen.html",
    "title": "Statistical Genetics",
    "section": "",
    "text": "Main work produced for this course was content summaries which involved summarizing important information from lectures and labs.\nProducts\nContent Summary 1: https://drive.google.com/file/d/1zYPC-N_uqZIixcNx64-OkYlHObDFYNum/view?usp=sharing\nContent Summary 2: Content Summary 2\nContent Summary 3: https://docs.google.com/document/d/1HM51evWTNX9X5kaDiijT-2u5ZDcuSovd/edit?usp=sharing&ouid=116188639352950574208&rtpof=true&sd=true"
  },
  {
    "objectID": "other/Bruce_Lee/bruce_lee.html",
    "href": "other/Bruce_Lee/bruce_lee.html",
    "title": "Racialization of Martial Arts in Bruce Lee Movies",
    "section": "",
    "text": "https://docs.google.com/document/d/1XQ2xhVYuWSEwiS3QNI7trlk-vv0sysk3/edit?usp=sharing&ouid=116188639352950574208&rtpof=true&sd=true"
  },
  {
    "objectID": "projects/CO2/CO2.html",
    "href": "projects/CO2/CO2.html",
    "title": "CO2 Levels in 20 Major Countries",
    "section": "",
    "text": "Completed as final project for STAT 253 “Statistical Machine Learning” at Macalester College with Ayaa Asoba and Julia Coelho.\nProducts\nPresentation: https://docs.google.com/presentation/d/1cRzZPT1LijIIKyGGKAEzkKDYWMsYUf22/edit?usp=sharing&ouid=116188639352950574208&rtpof=true&sd=true\nGithub: https://github.com/tchuang1290/StatMLFinalProj\nRMD: Final Code Appendix"
  },
  {
    "objectID": "projects/Renewable Energy/Renewable_Energy.html",
    "href": "projects/Renewable Energy/Renewable_Energy.html",
    "title": "America Going Green: A Look at Renewable Energy Generation in the U.S.",
    "section": "",
    "text": "Results produced as part of the class STAT 452 “Correlated Data” at Macalester College with Jack Acomb and Claire Wilson. Presentation produced separately as my senior capstone presentation.\nProducts\nPresentation: https://docs.google.com/presentation/d/1zKiCGBQ21L8hAcr4kaQXNjakufoih9xJnlrk9HtvWEQ/edit?usp=sharing"
  },
  {
    "objectID": "projects/BOLD/BOLD.html",
    "href": "projects/BOLD/BOLD.html",
    "title": "Energy and Pearson’s Distances as Metrics for Brain Region Activation Detection in Audiovisual fMRI Study",
    "section": "",
    "text": "This presentation was completed as part of the Summer Institute in Biostatistics at the University of Iowa and completed with Megan Gelement who was, at the time, a graduate student at Tufts University seeking her master’s degree in computer science.\nThe goal of this research was to determine measures of statistical distance that could be used to detect brain region activation. Since the brain is always active in some capacity, determining if a brain region is reacting to certain stimuli can be difficult.\nProducts:\nPresentation: https://docs.google.com/presentation/d/1kmFVMXeNdWhC2QxnQHdPnNxQeJv75aSqsETd-ZFTruk/edit?usp=sharing"
  },
  {
    "objectID": "projects/Bike/bike.html",
    "href": "projects/Bike/bike.html",
    "title": "Predicting Bike Share Demand in Seoul, South Korea",
    "section": "",
    "text": "Final project for STAT 5052 “Statistical and Machine Learning”. Completed with James Manne-Nicholas.\nAfter some light data exploration and cleaning, we fit and tuned several models like KNN regressors, random forest regressors, and gradient boosted trees in order to predict the amount of bikes that would be rented at a specific time in Seoul, South Korea. The best performing model was the random forest regressor although we believe with more time, the gradient boosted trees could have been more precisely tuned which may have resulted in better performance.\nProducts\nReport: https://docs.google.com/document/d/1cvEJvZvzSElZxPAhhu9ZaHuY_53BHdaD/edit?usp=sharing&ouid=116188639352950574208&rtpof=true&sd=true\nPresentation: https://docs.google.com/presentation/d/1wV44XmlXuoohZXDP1rqbFJv7o2USNWWD/edit?usp=sharing&ouid=116188639352950574208&rtpof=true&sd=true\nJupyter Notebook: https://github.com/tchuang1290/5052FinalProject/blob/main/5052Project.ipynb"
  },
  {
    "objectID": "posts/TidyTuesday1.html",
    "href": "posts/TidyTuesday1.html",
    "title": "Common Reasons for Terminating Himalayan Climbs (2020-2024)",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(forcats)\n\n\n# load data\nexped_tidy &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-21/exped_tidy.csv')\n\nRows: 882 Columns: 69\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (22): EXPID, PEAKID, SEASON_FACTOR, HOST_FACTOR, ROUTE1, ROUTE2, NATION...\ndbl  (17): YEAR, SEASON, HOST, SMTDAYS, TOTDAYS, TERMREASON, HIGHPOINT, CAMP...\nlgl  (27): ROUTE3, ROUTE4, SUCCESS1, SUCCESS2, SUCCESS3, SUCCESS4, ASCENT3, ...\ndate  (3): BCDATE, SMTDATE, TERMDATE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npeaks_tidy &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-01-21/peaks_tidy.csv')\n\nRows: 480 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (14): PEAKID, PKNAME, PKNAME2, LOCATION, HIMAL_FACTOR, REGION_FACTOR, RE...\ndbl (12): HEIGHTM, HEIGHTF, HIMAL, REGION, TREKYEAR, PHOST, PSTATUS, PEAKMEM...\nlgl  (3): OPEN, UNLISTED, TREKKING\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nexped_tidy %&gt;% \n  mutate(TERMREASON1 = as.factor(TERMREASON)) %&gt;% \n  filter(!TERMREASON1 %in% c(0,1,2)) %&gt;% \n  ggplot(aes(x = fct_infreq(TERMREASON1), fill = TERMREASON_FACTOR)) +\n  geom_bar() +\n  labs(title = \"Common Reasons for Terminating Himalayan Climbs (2020–2024)\",\n       fill = \"Reason for Termination\") +\n  scale_fill_discrete(breaks = c(\"Bad weather (storms, high winds)\",\"Bad conditions (deep snow, avalanching, falling ice, or rock)\", \"Illness, AMS, exhaustion, or frostbite\", \"Did not attempt climb\", \"Other\", \"Route technically too difficult, lack of experience, strength, or motivation\", \"Accident (death or serious injury)\", \"Lack (or loss) of supplies, support or equipment\", \"Lack of time\", \"Did not reach base camp\")) +\n  theme_classic() +\n  theme(axis.title = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        axis.line.x = element_blank())\n\n\n\n\n\n\n\n\nThe plot above displays common reasons Himalayan climbs were terminated between the years 2020 and 2024. The most common reason by far for a climb to be terminated was bad weather with almost 80 climbs stopped for this reason. The second most common reason was bad conditions like deep snow or avalanching with about 55 climbs terminated for this reason. The third most common reason being illness or other medical reasons not including serious injury or death which stopped over 20 climbs. The other listed reasons all stopped about 10 climbs or less with the least common reason for terminating a climb being that the group did not reach base camp.\nI reproduced this plot in Python using Pandas, Matplotlib, and Seaborn! Check it out here.\nData obtained from: Data Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day"
  },
  {
    "objectID": "projects/Personality/Personality.html",
    "href": "projects/Personality/Personality.html",
    "title": "GWAS and Personalities",
    "section": "",
    "text": "Produced for STAT 494 “Statistical Genetics” at Macalester College with Lucy Tran ’23.\nProducts\nPresentation: https://docs.google.com/presentation/d/1Ti9fU15B1p2mDweW9JadluguxhnSI7qk8W8jGvB5DH4/edit?usp=sharing\nDigital Artifact: https://sites.google.com/view/gwas-and-personalities/home?authuser=1"
  },
  {
    "objectID": "projects/Interference/Interference.html",
    "href": "projects/Interference/Interference.html",
    "title": "Interference in Causal Inference",
    "section": "",
    "text": "Final Project for STAT 451 “Causal Inference” at Macalester College.\nProduct\nReport: https://docs.google.com/document/d/18FHp-WNjlHc1ubSEVgNxdz8IuWxmmQr1/edit?usp=sharing&ouid=116188639352950574208&rtpof=true&sd=true"
  },
  {
    "objectID": "projects/Survival/Survival_Analysis.html",
    "href": "projects/Survival/Survival_Analysis.html",
    "title": "Analyzing Turbofan Jet Engines using Survival Analysis Methods",
    "section": "",
    "text": "Using Kaplan Meier curves and a Cox proportional hazard model, I analyzed the reliability of simulated turbo fan jet engines from data produced by Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) developed by NASA. This paper and presentation was produced as part of the class STAT 453 “Survival Analysis” at Macalester College.\nProducts\nPaper: https://docs.google.com/document/d/1yq_Jd_1LwriNe1Cd6GYp1IDszdTUs1tMx1HhQSqNwN0/edit\nPresentation: https://docs.google.com/presentation/d/1SZj3brdOlm7r6wHzMqbohh-ljHUzDvJ4sE0llfH7NKY/edit#slide=id.p\nRMD: Final Code Appendix"
  },
  {
    "objectID": "other/piano_recital/recital.html",
    "href": "other/piano_recital/recital.html",
    "title": "Senior Capstone-Piano Recital",
    "section": "",
    "text": "50 minute recital performed as my capstone project for my music major.\nProgram:\nEtude for Piano No 16 - Philip Glass\nPrelude and Fugue in C-Sharp Minor, Op. 87, No 10 - Dmitri Shostakovich\nSonata for Piano in E Major, Op. 109 - Ludwig Van Beethoven\nThe Reminiscents - Paul Gabriel L. Cosme\nBallade No. 3 in A-Flat Major, Op. 47 - Frederic Chopin\nWatch it here: https://vimeo.com/822164806/b3802f7e42"
  },
  {
    "objectID": "coursework/Machine Learning with Python and SciKit Learn/SciKit_Learn.html",
    "href": "coursework/Machine Learning with Python and SciKit Learn/SciKit_Learn.html",
    "title": "Machine Learning in Python with Sci-Kit Learn",
    "section": "",
    "text": "Online course through France Université Numérique. Coding in Python in Jupyter Notebooks while learning useful data science and data analysis packages like pandas, seaborn, and scikit-learn.\nGithub: https://github.com/tchuang1290/MLScikit-Learn/tree/main"
  },
  {
    "objectID": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html",
    "href": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html",
    "title": "Ayaa Asoba, Julia Coelho, Ting Huang stat253_project",
    "section": "",
    "text": "library(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(tidyr)\nlibrary(ggpubr)\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nWarning: package 'nlme' was built under R version 4.3.3\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tibble       3.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ nlme::collapse()  masks dplyr::collapse()\n✖ purrr::discard()  masks scales::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ dials::prune()    masks rpart::prune()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(probably) #install.packages('probably')\n\n\nAttaching package: 'probably'\n\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n\ntidymodels_prefer()\nesgdata &lt;- read_csv('ESGData.csv')\n\nNew names:\n• `` -&gt; `...67`\n\n\nRows: 16013 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Country Name, Country Code, Indicator Name, Indicator Code\ndbl (62): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nlgl  (1): ...67\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nesgdata %&gt;% distinct(`Indicator Name`,`Indicator Code`)\n\n# A tibble: 67 × 2\n   `Indicator Name`                                             `Indicator Code`\n   &lt;chr&gt;                                                        &lt;chr&gt;           \n 1 Access to clean fuels and technologies for cooking (% of po… EG.CFT.ACCS.ZS  \n 2 Access to electricity (% of population)                      EG.ELC.ACCS.ZS  \n 3 Adjusted savings: natural resources depletion (% of GNI)     NY.ADJ.DRES.GN.…\n 4 Adjusted savings: net forest depletion (% of GNI)            NY.ADJ.DFOR.GN.…\n 5 Agricultural land (% of land area)                           AG.LND.AGRI.ZS  \n 6 Agriculture, forestry, and fishing, value added (% of GDP)   NV.AGR.TOTL.ZS  \n 7 Annual freshwater withdrawals, total (% of internal resourc… ER.H2O.FWTL.ZS  \n 8 Annualized average growth rate in per capita real survey me… SI.SPR.PCAP.ZG  \n 9 Cause of death, by communicable diseases and maternal, pren… SH.DTH.COMM.ZS  \n10 Children in employment, total (% of children ages 7-14)      SL.TLF.0714.ZS  \n# ℹ 57 more rows"
  },
  {
    "objectID": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#reading-in-data",
    "href": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#reading-in-data",
    "title": "Ayaa Asoba, Julia Coelho, Ting Huang stat253_project",
    "section": "",
    "text": "library(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rpart.plot)\n\nLoading required package: rpart\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(tidyr)\nlibrary(ggpubr)\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nWarning: package 'nlme' was built under R version 4.3.3\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tibble       3.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ nlme::collapse()  masks dplyr::collapse()\n✖ purrr::discard()  masks scales::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ dials::prune()    masks rpart::prune()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(probably) #install.packages('probably')\n\n\nAttaching package: 'probably'\n\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n\ntidymodels_prefer()\nesgdata &lt;- read_csv('ESGData.csv')\n\nNew names:\n• `` -&gt; `...67`\n\n\nRows: 16013 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Country Name, Country Code, Indicator Name, Indicator Code\ndbl (62): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\nlgl  (1): ...67\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nesgdata %&gt;% distinct(`Indicator Name`,`Indicator Code`)\n\n# A tibble: 67 × 2\n   `Indicator Name`                                             `Indicator Code`\n   &lt;chr&gt;                                                        &lt;chr&gt;           \n 1 Access to clean fuels and technologies for cooking (% of po… EG.CFT.ACCS.ZS  \n 2 Access to electricity (% of population)                      EG.ELC.ACCS.ZS  \n 3 Adjusted savings: natural resources depletion (% of GNI)     NY.ADJ.DRES.GN.…\n 4 Adjusted savings: net forest depletion (% of GNI)            NY.ADJ.DFOR.GN.…\n 5 Agricultural land (% of land area)                           AG.LND.AGRI.ZS  \n 6 Agriculture, forestry, and fishing, value added (% of GDP)   NV.AGR.TOTL.ZS  \n 7 Annual freshwater withdrawals, total (% of internal resourc… ER.H2O.FWTL.ZS  \n 8 Annualized average growth rate in per capita real survey me… SI.SPR.PCAP.ZG  \n 9 Cause of death, by communicable diseases and maternal, pren… SH.DTH.COMM.ZS  \n10 Children in employment, total (% of children ages 7-14)      SL.TLF.0714.ZS  \n# ℹ 57 more rows"
  },
  {
    "objectID": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#data-cleaning",
    "href": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#data-cleaning",
    "title": "Ayaa Asoba, Julia Coelho, Ting Huang stat253_project",
    "section": "Data cleaning",
    "text": "Data cleaning\n\n# data cleaning\nesgdata_clean &lt;- filter(esgdata,`Indicator Code`%in% c(\"EN.ATM.CO2E.PC\",\"EG.FEC.RNEW.ZS\", \"EG.ELC.RNEW.ZS\",\"EG.EGY.PRIM.PP.KD\",\"EG.USE.COMM.FO.ZS\",\"EN.POP.DNST\",\"AG.LND.AGRI.ZS\",\"NV.AGR.TOTL.ZS\",\"EG.ELC.ACCS.ZS\",\"SP.DYN.LE00.IN\",\"SI.DST.FRST.20\",\"SI.POV.GINI\",\"IT.NET.USER.ZS\",\"GE.EST\",\"AG.LND.FRST.ZS\",\"GB.XPD.RSDV.GD.ZS\",\"SE.PRM.ENRR\"))\nesgdata_clean &lt;- select(esgdata_clean,'Country Code','Country Name','Indicator Name','Indicator Code','1991','1992','1993','1994','1995','1996','1997','1998','1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018')\nesgdataAll &lt;- esgdata_clean %&gt;%\n  rename(CountryCode=\"Country Code\") %&gt;% rename(CountryName=\"Country Name\") %&gt;% rename(IndicatorName=\"Indicator Name\") %&gt;% rename(IndicatorCode=\"Indicator Code\")\nesgdataAll &lt;- esgdataAll %&gt;%\n  filter(!CountryCode %in% c(\"ARB\",\"CEB\",\"CSS\",\"ECA\",\"EAP\",\"EAR\",\"EAS\",\"ECS\",\"EMU\",\"EUU\",\"FCS\",\"HIC\",\"HPC\",\"IBD\",\"IBT\",\"IDA\",\"IDB\",\"IDX\",\"LDC\",\"LCN\",\"LIC\",\"LAC\",\"LMC\",\"LMY\",\"LTE\",\"MIC\",\"MEA\",\"MNA\",\"NAC\",\"OED\",\"OSS\",\"PRE\",\"PSS\",\"PST\",\"SAS\",\"SSA\",\"SSF\",\"SST\",\"TEA\",\"TMN\",\"TEC\",\"TLA\",\"TSA\",\"TSS\",\"UMC\",\"WLD\")) %&gt;%\n  pivot_longer(-c('CountryCode','CountryName','IndicatorName','IndicatorCode'), names_to = 'Year', values_to = 'Values') %&gt;% \n  select(-`IndicatorName`) %&gt;% \n  pivot_wider(names_from='IndicatorCode',values_from = 'Values')\nesgdataAll &lt;- esgdataAll %&gt;%\n   rename(electricity=\"EG.ELC.ACCS.ZS\") %&gt;% rename(agroLand=\"AG.LND.AGRI.ZS\") %&gt;% rename(agroValue=\"NV.AGR.TOTL.ZS\") %&gt;% rename(co2=\"EN.ATM.CO2E.PC\") %&gt;% rename(energyIntensity=\"EG.EGY.PRIM.PP.KD\") %&gt;% rename(forestArea=\"AG.LND.FRST.ZS\") %&gt;% rename(fossilFuel=\"EG.USE.COMM.FO.ZS\") %&gt;% rename(gini=\"SI.POV.GINI\") %&gt;% rename(govtEfficacy=\"GE.EST\") %&gt;% rename(incomeLowest20=\"SI.DST.FRST.20\") %&gt;% rename(internet=\"IT.NET.USER.ZS\") %&gt;% rename(lifeExpectancy=\"SP.DYN.LE00.IN\")  %&gt;% rename(popDensity=\"EN.POP.DNST\") %&gt;% rename(renewableElec=\"EG.ELC.RNEW.ZS\") %&gt;% rename(renewableEnergy=\"EG.FEC.RNEW.ZS\") %&gt;% rename(research=\"GB.XPD.RSDV.GD.ZS\") %&gt;% rename(schoolEnroll=\"SE.PRM.ENRR\")\nesgdataAll &lt;- esgdataAll %&gt;%\n  mutate(Year = as.numeric(Year))\nesgdataAll_sub &lt;- esgdataAll %&gt;% na.omit()\nesgdataAll_sub %&gt;% purrr::map (~sum(is.na(.)))\n\n$CountryCode\n[1] 0\n\n$CountryName\n[1] 0\n\n$Year\n[1] 0\n\n$electricity\n[1] 0\n\n$agroLand\n[1] 0\n\n$agroValue\n[1] 0\n\n$co2\n[1] 0\n\n$energyIntensity\n[1] 0\n\n$forestArea\n[1] 0\n\n$fossilFuel\n[1] 0\n\n$gini\n[1] 0\n\n$govtEfficacy\n[1] 0\n\n$incomeLowest20\n[1] 0\n\n$internet\n[1] 0\n\n$lifeExpectancy\n[1] 0\n\n$popDensity\n[1] 0\n\n$renewableElec\n[1] 0\n\n$renewableEnergy\n[1] 0\n\n$research\n[1] 0\n\n$schoolEnroll\n[1] 0\n\nesgdata_cv10 &lt;- vfold_cv(esgdataAll_sub, v = 10)  # 6 folds\ndata_rec &lt;- recipe(co2 ~ . , data = esgdataAll_sub) %&gt;%\n  update_role(`CountryCode`,new_role = 'ID') %&gt;%\n  update_role(`CountryName`,new_role = 'ID') %&gt;%\n    step_nzv(all_predictors()) %&gt;% # removes variables with the same value\n    step_normalize(all_numeric_predictors()) %&gt;%  # important standardization step for LASSO\n    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables\ndata_rec %&gt;% prep(esgdataAll_sub) %&gt;%juice()\n\n# A tibble: 713 × 20\n   CountryCode CountryName    Year electricity agroLand agroValue\n   &lt;fct&gt;       &lt;fct&gt;         &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 ALB         Albania     -0.0415     0.291    -0.0194    1.94  \n 2 ARG         Argentina   -2.77      -0.304     0.170     0.0142\n 3 ARG         Argentina   -2.32      -0.238     0.173    -0.0472\n 4 ARG         Argentina   -1.86      -0.174     0.176    -0.145 \n 5 ARG         Argentina   -1.41      -0.112     0.180     0.802 \n 6 ARG         Argentina   -1.18      -0.0831    0.220     0.823 \n 7 ARG         Argentina   -0.952     -0.0548    0.285     0.486 \n 8 ARG         Argentina   -0.724     -0.0278    0.349     0.413 \n 9 ARG         Argentina   -0.497     -0.00166   0.410     0.241 \n10 ARG         Argentina   -0.269      0.0241    0.464     0.330 \n# ℹ 703 more rows\n# ℹ 14 more variables: energyIntensity &lt;dbl&gt;, forestArea &lt;dbl&gt;,\n#   fossilFuel &lt;dbl&gt;, gini &lt;dbl&gt;, govtEfficacy &lt;dbl&gt;, incomeLowest20 &lt;dbl&gt;,\n#   internet &lt;dbl&gt;, lifeExpectancy &lt;dbl&gt;, popDensity &lt;dbl&gt;,\n#   renewableElec &lt;dbl&gt;, renewableEnergy &lt;dbl&gt;, research &lt;dbl&gt;,\n#   schoolEnroll &lt;dbl&gt;, co2 &lt;dbl&gt;"
  },
  {
    "objectID": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#regression-models",
    "href": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#regression-models",
    "title": "Ayaa Asoba, Julia Coelho, Ting Huang stat253_project",
    "section": "Regression Models",
    "text": "Regression Models\n\nLinear Model Recipe\n\nesg_rec &lt;- recipe(co2 ~ . , data = esgdataAll_sub) %&gt;%\n  update_role(`Year`,new_role = 'ID') %&gt;%\n  update_role(`CountryCode`,new_role = 'ID') %&gt;%\n  update_role(`CountryName`,new_role = 'ID') %&gt;%\n    step_nzv(all_predictors()) %&gt;% # removes variables with the same value\n    step_normalize(all_numeric_predictors()) %&gt;%  # important standardization step for LASSO\n    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables\n    \nesg_rec %&gt;% prep(esgdataAll_sub) %&gt;% juice()\n\n# A tibble: 713 × 20\n   CountryCode CountryName  Year electricity agroLand agroValue energyIntensity\n   &lt;fct&gt;       &lt;fct&gt;       &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 ALB         Albania      2008     0.291    -0.0194    1.94            -0.640\n 2 ARG         Argentina    1996    -0.304     0.170     0.0142          -0.324\n 3 ARG         Argentina    1998    -0.238     0.173    -0.0472          -0.388\n 4 ARG         Argentina    2000    -0.174     0.176    -0.145           -0.298\n 5 ARG         Argentina    2002    -0.112     0.180     0.802           -0.186\n 6 ARG         Argentina    2003    -0.0831    0.220     0.823           -0.210\n 7 ARG         Argentina    2004    -0.0548    0.285     0.486           -0.207\n 8 ARG         Argentina    2005    -0.0278    0.349     0.413           -0.314\n 9 ARG         Argentina    2006    -0.00166   0.410     0.241           -0.306\n10 ARG         Argentina    2007     0.0241    0.464     0.330           -0.398\n# ℹ 703 more rows\n# ℹ 13 more variables: forestArea &lt;dbl&gt;, fossilFuel &lt;dbl&gt;, gini &lt;dbl&gt;,\n#   govtEfficacy &lt;dbl&gt;, incomeLowest20 &lt;dbl&gt;, internet &lt;dbl&gt;,\n#   lifeExpectancy &lt;dbl&gt;, popDensity &lt;dbl&gt;, renewableElec &lt;dbl&gt;,\n#   renewableEnergy &lt;dbl&gt;, research &lt;dbl&gt;, schoolEnroll &lt;dbl&gt;, co2 &lt;dbl&gt;\n\n\n\n\nLinear Model Fit\n\nlm_spec &lt;- \n    linear_reg() %&gt;% \n    set_engine(engine = 'lm') %&gt;% \n    set_mode('regression')\nesg_model_wf1 &lt;- workflow() %&gt;%\n  add_recipe(esg_rec) %&gt;% \n  add_model(lm_spec)\n \nesg_fit_model1 &lt;- esg_model_wf1 %&gt;% \n  fit(data = esgdataAll_sub)\nesg_fit_model1 %&gt;% tidy() \n\n# A tibble: 17 × 5\n   term            estimate std.error statistic   p.value\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)       6.55       0.105    62.3   4.59e-287\n 2 electricity       0.0221     0.170     0.130 8.97e-  1\n 3 agroLand         -0.143      0.146    -0.975 3.30e-  1\n 4 agroValue        -0.842      0.200    -4.21  2.86e-  5\n 5 energyIntensity   1.14       0.148     7.72  4.09e- 14\n 6 forestArea        0.264      0.161     1.64  1.00e-  1\n 7 fossilFuel        0.775      0.212     3.65  2.77e-  4\n 8 gini             -0.497      0.460    -1.08  2.81e-  1\n 9 govtEfficacy      2.18       0.251     8.69  2.52e- 17\n10 incomeLowest20   -0.408      0.433    -0.944 3.46e-  1\n11 internet          0.425      0.204     2.08  3.75e-  2\n12 lifeExpectancy   -0.835      0.240    -3.49  5.21e-  4\n13 popDensity       -0.492      0.123    -3.99  7.46e-  5\n14 renewableElec    -0.119      0.220    -0.541 5.89e-  1\n15 renewableEnergy  -1.20       0.306    -3.93  9.53e-  5\n16 research          0.477      0.187     2.55  1.10e-  2\n17 schoolEnroll     -0.185      0.134    -1.39  1.66e-  1\n\n\n\n# Getting metrics\ncv_output &lt;- fit_resamples( # new function for tuning parameters\n  esg_model_wf1, # workflow\n  resamples = esgdata_cv10, # cv folds\n  metrics = metric_set(rmse, mae, rsq)\n)\ncv_output %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   1.99     10  0.0972 Preprocessor1_Model1\n2 rmse    standard   2.79     10  0.178  Preprocessor1_Model1\n3 rsq     standard   0.631    10  0.0262 Preprocessor1_Model1\n\n# Residuals \nesg_fit_model1_residuals &lt;- bind_cols(esgdataAll_sub, esg_fit_model1 %&gt;% \n  predict(new_data = esgdataAll_sub)) %&gt;%\n  mutate(resid = co2 - .pred)\nggplot(esg_fit_model1_residuals, aes(x = .pred, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    ggtitle(\"Linear Regression Residuals\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()    # fit to the training data \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nLASSO Model\n\ndata_rec &lt;- recipe(co2 ~ . , data = esgdataAll_sub) %&gt;%\n  update_role(`Year`,new_role = 'ID') %&gt;%\n  update_role(`CountryCode`,new_role = 'ID') %&gt;%\n  update_role(`CountryName`,new_role = 'ID') %&gt;%\n    step_nzv(all_predictors()) %&gt;% # removes variables with the same value\n    step_normalize(all_numeric_predictors()) %&gt;%  # important standardization step for LASSO\n    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables\n# Lasso Model Spec with tune\nlm_lasso_spec_tune &lt;- \n  linear_reg() %&gt;%\n  set_args(mixture = 1, penalty = tune()) %&gt;% ## mixture = 1 indicates Lasso\n  set_engine(engine = 'glmnet') %&gt;% #note we are using a different engine\n  set_mode('regression') \n# Workflow (Recipe + Model)\nlasso_wf_tune &lt;- workflow() %&gt;% \n  add_recipe(data_rec) %&gt;%\n  add_model(lm_lasso_spec_tune) \n# Tune Model (trying a variety of values of Lambda penalty)\npenalty_grid &lt;- grid_regular(\n  penalty(range = c(-3, 1)), #log10 transformed \n  levels = 30)\ntune_output &lt;- tune_grid( # new function for tuning hyperparameters\n  lasso_wf_tune, # workflow\n  resamples = esgdata_cv10, # cv folds\n  metrics = metric_set(rmse, mae),\n  grid = penalty_grid # penalty grid defined above\n)\nautoplot(tune_output) + theme_classic()\n\n\n\n\n\n\n\n\n\nPicking LASSO Penalty\n\nbest_penalty &lt;- select_best(tune_output, metric = 'mae') # choose penalty value based on lowest mae\nbest_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1  0.0621 Preprocessor1_Model14\n\nbest_se_penalty &lt;- select_by_one_std_err(tune_output, metric = 'mae', desc(penalty)) # choose penalty value based on the largest penalty within 1 se of the lowest CV MAE\nbest_se_penalty\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.304 Preprocessor1_Model19\n\nfinal_wf &lt;- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow\nfinal_wf_se &lt;- finalize_workflow(lasso_wf_tune, best_se_penalty) # incorporates penalty value to workflow\nfinal_fit &lt;- fit(final_wf, data = esgdataAll_sub)\nfinal_fit_se &lt;- fit(final_wf_se, data = esgdataAll_sub)\ntidy(final_fit)\n\n# A tibble: 17 × 3\n   term            estimate penalty\n   &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)       6.55    0.0621\n 2 electricity       0       0.0621\n 3 agroLand         -0.0705  0.0621\n 4 agroValue        -0.855   0.0621\n 5 energyIntensity   1.10    0.0621\n 6 forestArea        0.178   0.0621\n 7 fossilFuel        0.582   0.0621\n 8 gini             -0.0429  0.0621\n 9 govtEfficacy      1.89    0.0621\n10 incomeLowest20    0       0.0621\n11 internet          0.284   0.0621\n12 lifeExpectancy   -0.413   0.0621\n13 popDensity       -0.438   0.0621\n14 renewableElec    -0.199   0.0621\n15 renewableEnergy  -1.15    0.0621\n16 research          0.507   0.0621\n17 schoolEnroll     -0.219   0.0621\n\ntidy(final_fit_se)\n\n# A tibble: 17 × 3\n   term             estimate penalty\n   &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      6.55       0.304\n 2 electricity      0          0.304\n 3 agroLand         0          0.304\n 4 agroValue       -0.751      0.304\n 5 energyIntensity  0.760      0.304\n 6 forestArea       0          0.304\n 7 fossilFuel       0.000135   0.304\n 8 gini             0          0.304\n 9 govtEfficacy     1.36       0.304\n10 incomeLowest20   0          0.304\n11 internet         0          0.304\n12 lifeExpectancy   0          0.304\n13 popDensity      -0.115      0.304\n14 renewableElec   -0.241      0.304\n15 renewableEnergy -1.18       0.304\n16 research         0.598      0.304\n17 schoolEnroll    -0.167      0.304\n\n\n\nglmnet_output &lt;- final_fit_se %&gt;% extract_fit_parsnip() %&gt;% pluck('fit') # way to get the original glmnet output\nlambdas &lt;- glmnet_output$lambda\ncoefs_lambdas &lt;- \n  coefficients(glmnet_output, s = lambdas )  %&gt;% \n  as.matrix() %&gt;%  \n  t() %&gt;% \n  as.data.frame() %&gt;% \n  mutate(lambda = lambdas ) %&gt;% \n  select(lambda, everything(), -`(Intercept)`) %&gt;% \n  pivot_longer(cols = -lambda, \n               names_to = \"term\", \n               values_to = \"coef\") %&gt;%\n  mutate(var = map_chr(stringr::str_split(term,\"_\"),~.[1]))\ncoefs_lambdas %&gt;%\n  ggplot(aes(x = lambda, y = coef, group = term, color = var)) +\n  geom_line() +\n  geom_vline(xintercept = best_se_penalty %&gt;% pull(penalty), linetype = 'dashed') + \n  theme_classic() + \n  theme(legend.position = \"bottom\", legend.text=element_text(size=8))\n\n\n\n\n\n\n\n# Create a boolean matrix (predictors x lambdas) of variable exclusion\nbool_predictor_exclude &lt;- glmnet_output$beta==0\n# Loop over each variable\nvar_imp &lt;- sapply(seq_len(nrow(bool_predictor_exclude)), function(row) {\n    this_coeff_path &lt;- bool_predictor_exclude[row,]\n    if(sum(this_coeff_path) == ncol(bool_predictor_exclude)){ return(0)}else{\n    return(ncol(bool_predictor_exclude) - which.min(this_coeff_path) + 1)}\n})\n# Create a dataset of this information and sort\nvar_imp_data &lt;- tibble(\n    var_name = rownames(bool_predictor_exclude),\n    var_imp = var_imp\n)\nvar_imp_data %&gt;% arrange(desc(var_imp))\n\n# A tibble: 16 × 2\n   var_name        var_imp\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 govtEfficacy         75\n 2 agroValue            74\n 3 renewableElec        73\n 4 renewableEnergy      73\n 5 research             73\n 6 energyIntensity      63\n 7 gini                 63\n 8 schoolEnroll         60\n 9 popDensity           55\n10 fossilFuel           53\n11 internet             52\n12 forestArea           46\n13 agroLand             44\n14 lifeExpectancy       44\n15 incomeLowest20       18\n16 electricity          10\n\n\n\nfinal_fit_se %&gt;% tidy() %&gt;% filter(estimate != 0)\n\n# A tibble: 10 × 3\n   term             estimate penalty\n   &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      6.55       0.304\n 2 agroValue       -0.751      0.304\n 3 energyIntensity  0.760      0.304\n 4 fossilFuel       0.000135   0.304\n 5 govtEfficacy     1.36       0.304\n 6 popDensity      -0.115      0.304\n 7 renewableElec   -0.241      0.304\n 8 renewableEnergy -1.18       0.304\n 9 research         0.598      0.304\n10 schoolEnroll    -0.167      0.304\n\ntune_output %&gt;% collect_metrics() %&gt;% filter(penalty == (best_se_penalty %&gt;% pull(penalty)))\n\n# A tibble: 2 × 7\n  penalty .metric .estimator  mean     n std_err .config              \n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1   0.304 mae     standard    2.04    10   0.111 Preprocessor1_Model19\n2   0.304 rmse    standard    2.91    10   0.204 Preprocessor1_Model19\n\nlasso_mod_out &lt;- final_fit_se %&gt;%\n    predict(new_data = esgdataAll_sub) %&gt;%\n    bind_cols(esgdataAll_sub) %&gt;%\n    mutate(resid = co2 - .pred)\nggplot(esg_fit_model1_residuals, aes(x = .pred, y = resid)) +\n    geom_point() +\n    ggtitle('Linear Regression Final Model') +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()    # fit to the training data \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nlasso_mod_out %&gt;% \n  ggplot(aes(x = .pred, y = resid)) + \n  ggtitle('LASSO Residuals') +\n  geom_point() +\n  geom_smooth(se = FALSE) + \n  geom_hline(yintercept = 0, color = \"red\") + \n  theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nGAMs\n\ngam_spec &lt;- \n  gen_additive_mod() %&gt;%\n  set_engine(engine = 'mgcv') %&gt;%\n  set_mode('regression') \ngam_mod &lt;- fit(gam_spec,\n    co2 ~ s(agroValue) + s(energyIntensity) + s(fossilFuel) + s(govtEfficacy) + s(popDensity) + s(renewableElec) + s(research) + s(schoolEnroll),\n    data = esgdataAll_sub\n)\n\n\ngam_mod %&gt;% pluck('fit') %&gt;% plot( all.terms = TRUE, pages = 1)\n\n\n\n\n\n\n\ngam_mod %&gt;% pluck('fit') %&gt;% summary() \n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nco2 ~ s(agroValue) + s(energyIntensity) + s(fossilFuel) + s(govtEfficacy) + \n    s(popDensity) + s(renewableElec) + s(research) + s(schoolEnroll)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.55010    0.06926   94.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                     edf Ref.df      F  p-value    \ns(agroValue)       8.600  8.948 13.878  &lt; 2e-16 ***\ns(energyIntensity) 8.801  8.985 27.180  &lt; 2e-16 ***\ns(fossilFuel)      8.468  8.909 11.883  &lt; 2e-16 ***\ns(govtEfficacy)    7.026  8.134  9.503  &lt; 2e-16 ***\ns(popDensity)      8.957  8.999 13.224  &lt; 2e-16 ***\ns(renewableElec)   7.970  8.705  5.465 1.28e-06 ***\ns(research)        8.182  8.817  9.380  &lt; 2e-16 ***\ns(schoolEnroll)    7.599  8.511  5.386 1.24e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.831   Deviance explained = 84.6%\nGCV = 3.7724  Scale est. = 3.42      n = 713\n\n\n\ngam_data_rec &lt;- recipe(co2 ~ agroValue + energyIntensity + fossilFuel + govtEfficacy + popDensity + renewableElec + research + schoolEnroll , data = esgdataAll_sub) %&gt;%\n    step_nzv(all_predictors()) %&gt;% # removes variables with the same value\n    step_dummy(all_nominal_predictors()) %&gt;%  # creates indicator variables for categorical variables\n     step_ns(agroValue, deg_free = 9) %&gt;% \n     step_ns(energyIntensity, deg_free = 9) %&gt;%\n     step_ns(fossilFuel, deg_free = 8) %&gt;%\n     step_ns(govtEfficacy, deg_free = 7) %&gt;%\n     step_ns(popDensity, deg_free = 9) %&gt;%\n     step_ns(renewableElec, deg_free = 8) %&gt;%\n     step_ns(research, deg_free = 8) %&gt;% \n    step_ns(schoolEnroll, deg_free = 8)\nspline_wf &lt;- workflow() %&gt;%\n    add_model(lm_spec) %&gt;%\n    add_recipe(gam_data_rec)\nfit_resamples(\n    spline_wf ,\n    resamples = esgdata_cv10, # cv folds\n    metrics = metric_set(mae,rsq)                     \n) %&gt;% collect_metrics()\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   1.40     10  0.0368 Preprocessor1_Model1\n2 rsq     standard   0.822    10  0.0133 Preprocessor1_Model1\n\nesg_gam_model2 &lt;- spline_wf %&gt;% fit(data=esgdataAll_sub)\nesg_gam_model2_residuals &lt;- bind_cols(esgdataAll_sub, esg_gam_model2 %&gt;% \n  predict(new_data = esgdataAll_sub)) %&gt;%\n  mutate(resid = co2 - .pred)\nresid_agro &lt;- ggplot(esg_gam_model2_residuals, aes(x = agroValue, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()   \nresid_energy &lt;- ggplot(esg_gam_model2_residuals, aes(x = energyIntensity, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic() \nresid_fossil &lt;- ggplot(esg_gam_model2_residuals, aes(x = fossilFuel, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()\nresid_govt &lt;- ggplot(esg_gam_model2_residuals, aes(x = govtEfficacy, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()\nresid_density &lt;- ggplot(esg_gam_model2_residuals, aes(x = popDensity, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()\nresid_renewable &lt;- ggplot(esg_gam_model2_residuals, aes(x = renewableElec, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()\nresid_research &lt;- ggplot(esg_gam_model2_residuals, aes(x = research, y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()\nresid_school &lt;- ggplot(esg_gam_model2_residuals, aes(x = agroValue , y = resid)) +\n    geom_point() +\n    geom_smooth() +\n    geom_hline(yintercept = 0, color = \"red\") +\n    labs(x = \"Fitted values\", y = \"Residuals\") +\n    theme_classic()\nggplot(esg_gam_model2_residuals, aes(x = resid, y = co2)) +\n    geom_point(alpha = 0.25) +\n    geom_smooth(color = \"blue\", se = FALSE) +\n    theme_classic()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggarrange(resid_agro, resid_energy, resid_fossil, resid_govt, resid_density, resid_renewable, resid_research, resid_school + rremove(\"x.text\"), \n          labels = c(\"agroValue Residuals\", \"energyIntensity Residuals\", \"fossilFuel Residuals\", \"govtEfficacy Residuals\", \"popDensity Residuals\", \"renewableElec Residuals\", \"research Residuals\", \"schoolEnroll Residuals\"),\n          ncol = 4, nrow = 2)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#classification-models",
    "href": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#classification-models",
    "title": "Ayaa Asoba, Julia Coelho, Ting Huang stat253_project",
    "section": "Classification Models",
    "text": "Classification Models\n\nmean(esgdataAll_sub[[\"co2\"]])\n\n[1] 6.550097\n\nesgdataAll_sub &lt;- esgdataAll_sub %&gt;%\n  mutate(CO2cat = if_else(esgdataAll_sub$co2 &gt; 6.550097, 'high_co2','low_co2'))\nesgdataAll_sub &lt;- esgdataAll_sub %&gt;%\n  mutate(CO2cat = relevel(factor(CO2cat), ref= 'low_co2'))\n\n\nWe will be using 6.550097 as the split for deciding if the country has high CO2 emissions or low CO2 emissions. ### Logistic Regression\n\n\n# Logistic Regression Model Spec\n\nlogistic_spec &lt;- logistic_reg() %&gt;%\n    set_engine('glm') %&gt;%\n    set_mode('classification')\n    \n# Recipe\n logistic_rec &lt;- recipe( CO2cat ~ ., data = esgdataAll_sub) %&gt;%\n    update_role(`CountryCode`,new_role = 'ID') %&gt;%\n     update_role(`CountryName`,new_role = 'ID') %&gt;%\n     step_rm(co2) %&gt;%\n     step_nzv(all_predictors()) %&gt;% # removes variables with the same value\n     step_normalize(all_numeric_predictors()) %&gt;%  # important standardization step for LASSO\n     step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables\n# Workflow (Recipe + Model)\n\nlog_wf &lt;- workflow() %&gt;% \n    add_recipe(logistic_rec) %&gt;%\n    add_model(logistic_spec) \n# Fit Model to Training Data\nlog_fit &lt;- fit(log_wf, data = esgdataAll_sub)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nExamining the logistic model\n\n# Print out Coefficients\nlog_fit %&gt;% tidy()\n\n# A tibble: 18 × 5\n   term            estimate std.error statistic  p.value\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)       -2.16      0.351    -6.16  7.38e-10\n 2 Year              -0.770     0.276    -2.79  5.32e- 3\n 3 electricity        0.517     1.09      0.476 6.34e- 1\n 4 agroLand          -0.654     0.260    -2.51  1.21e- 2\n 5 agroValue         -3.62      0.566    -6.38  1.72e-10\n 6 energyIntensity    1.77      0.287     6.17  6.68e-10\n 7 forestArea        -0.234     0.239    -0.979 3.27e- 1\n 8 fossilFuel         2.04      0.288     7.09  1.30e-12\n 9 gini              -0.431     0.744    -0.580 5.62e- 1\n10 govtEfficacy       0.954     0.416     2.29  2.19e- 2\n11 incomeLowest20    -0.484     0.640    -0.757 4.49e- 1\n12 internet           1.23      0.437     2.82  4.84e- 3\n13 lifeExpectancy    -0.357     0.286    -1.25  2.12e- 1\n14 popDensity        -0.794     0.164    -4.85  1.25e- 6\n15 renewableElec     -0.624     0.404    -1.54  1.23e- 1\n16 renewableEnergy   -0.392     0.511    -0.767 4.43e- 1\n17 research           0.531     0.267     1.99  4.70e- 2\n18 schoolEnroll      -0.616     0.223    -2.75  5.88e- 3\n\n# Get Exponentiated coefficients + CI\nlog_fit %&gt;% tidy() %&gt;%\n  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %&gt;% # do this first\n  mutate(OR = exp(estimate))\n\n# A tibble: 18 × 8\n   term    estimate std.error statistic  p.value OR.conf.low OR.conf.high     OR\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 (Inter…   -2.16      0.351    -6.16  7.38e-10     0.0578        0.229  0.115 \n 2 Year      -0.770     0.276    -2.79  5.32e- 3     0.269         0.796  0.463 \n 3 electr…    0.517     1.09      0.476 6.34e- 1     0.200        14.1    1.68  \n 4 agroLa…   -0.654     0.260    -2.51  1.21e- 2     0.312         0.866  0.520 \n 5 agroVa…   -3.62      0.566    -6.38  1.72e-10     0.00886       0.0816 0.0269\n 6 energy…    1.77      0.287     6.17  6.68e-10     3.36         10.4    5.90  \n 7 forest…   -0.234     0.239    -0.979 3.27e- 1     0.495         1.26   0.791 \n 8 fossil…    2.04      0.288     7.09  1.30e-12     4.39         13.6    7.71  \n 9 gini      -0.431     0.744    -0.580 5.62e- 1     0.151         2.79   0.650 \n10 govtEf…    0.954     0.416     2.29  2.19e- 2     1.15          5.87   2.60  \n11 income…   -0.484     0.640    -0.757 4.49e- 1     0.176         2.16   0.616 \n12 intern…    1.23      0.437     2.82  4.84e- 3     1.45          8.05   3.42  \n13 lifeEx…   -0.357     0.286    -1.25  2.12e- 1     0.400         1.23   0.700 \n14 popDen…   -0.794     0.164    -4.85  1.25e- 6     0.328         0.623  0.452 \n15 renewa…   -0.624     0.404    -1.54  1.23e- 1     0.243         1.18   0.536 \n16 renewa…   -0.392     0.511    -0.767 4.43e- 1     0.248         1.84   0.676 \n17 resear…    0.531     0.267     1.99  4.70e- 2     1.01          2.87   1.70  \n18 school…   -0.616     0.223    -2.75  5.88e- 3     0.349         0.837  0.540 \n\n\n\nlogistic_output &lt;-  esgdataAll_sub %&gt;%\n  bind_cols(predict(log_fit, new_data = esgdataAll_sub, type = 'prob')) \n# Hard predictions (you pick threshold)\nlogistic_output &lt;- logistic_output %&gt;%\n  mutate(.pred_class = make_two_class_pred(.pred_low_co2, levels(CO2cat), threshold = .55)) #Try changing threshold (.5, 0, 1, .2, .8)\n# Visualize Soft Predictions\nlogistic_output %&gt;%\n  ggplot(aes(x = CO2cat, y = .pred_low_co2)) +\n  geom_boxplot() + \n  geom_hline(yintercept = 0.55, color='red') +  # try changing threshold\n  labs(y = 'Predicted Probability of Low CO2', x = 'Observed Outcome') +\n  theme_classic()\n\n\n\n\n\n\n\nlogistic_output %&gt;%\n  conf_mat(truth = CO2cat, estimate = .pred_class)\n\n          Truth\nPrediction low_co2 high_co2\n  low_co2      357       39\n  high_co2      45      272\n\nlog_metrics &lt;- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions\n#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)\n#spec: specificity = chance of correctly predicting first level, given first level (No)\n#accuracy: accuracy = chance of correctly predicting outcome\nlogistic_output %&gt;% \n   log_metrics(estimate = .pred_class, truth = CO2cat, event_level = \"second\") # set second level of outcome as \"success\"\n\n# A tibble: 3 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 sens     binary         0.875\n2 spec     binary         0.888\n3 accuracy binary         0.882\n\n\n\n logistic_output &lt;-  esgdataAll_sub %&gt;%\n   bind_cols(predict(log_fit, new_data = esgdataAll_sub, type = 'prob')) \n logistic_roc &lt;- logistic_output %&gt;% \n     roc_curve(CO2cat, .pred_high_co2, event_level = \"second\") # set second level of outcome as \"success\"\n autoplot(logistic_roc) + theme_classic()\n\n\n\n\n\n\n\n set.seed(123)\n esgdataAll_sub_cv10 &lt;- vfold_cv(esgdataAll_sub, v = 10)\n # CV Fit Model\n log_cv_fit &lt;- fit_resamples(\n     log_wf, \n     resamples = esgdataAll_sub_cv10,\n     metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),\n     control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations\n\n→ A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x10\n\n\n\n\n collect_metrics(log_cv_fit) #default threshold is 0.5\n\n# A tibble: 4 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.864    10 0.0128  Preprocessor1_Model1\n2 roc_auc  binary     0.943    10 0.00729 Preprocessor1_Model1\n3 sens     binary     0.843    10 0.0250  Preprocessor1_Model1\n4 spec     binary     0.882    10 0.0139  Preprocessor1_Model1\n\n\n\nDecision tree\n\nset.seed(726) # don't change this\nct_spec_tune &lt;- decision_tree() %&gt;%\n  set_engine(engine = 'rpart') %&gt;%\n  set_args(cost_complexity = tune(),  \n           min_n = 2, \n           tree_depth = NULL) %&gt;% \n  set_mode('classification') \ndata_rec &lt;- recipe(CO2cat ~ ., data = esgdataAll_sub) %&gt;%\n    update_role(`CountryCode`,new_role = 'ID') %&gt;%\n    update_role(`CountryName`,new_role = 'ID') %&gt;%\n    step_rm(co2) %&gt;%\n    step_nzv(all_predictors()) %&gt;% # removes variables with the same value\n    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables\ndata_wf_tune &lt;- workflow() %&gt;%\n  add_model(ct_spec_tune) %&gt;%\n  add_recipe(data_rec)\nparam_grid &lt;- grid_regular(cost_complexity(range = c(-5, 1)), levels = 10) \ntune_res &lt;- tune_grid(\n  data_wf_tune, \n  resamples = esgdataAll_sub_cv10, \n  grid = param_grid, \n  metrics = metric_set(accuracy) #change this for regression trees\n)\n\n\nautoplot(tune_res) + theme_classic()\n\n\n\n\n\n\n\n\n\nbest_complexity &lt;- select_by_one_std_err(tune_res, metric = 'accuracy', desc(cost_complexity))\ndata_wf_final &lt;- finalize_workflow(data_wf_tune, best_complexity)\nesg_final_fit &lt;- fit(data_wf_final, data = esgdataAll_sub)\n\n\nesg_final_fit %&gt;% extract_fit_engine() %&gt;% rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\n\ntree_mod_highcp &lt;- fit(\n    data_wf_tune %&gt;%\n        update_model(ct_spec_tune %&gt;% set_args(cost_complexity = .01)),\n    data = esgdataAll_sub\n)\ntree_mod_highcp %&gt;% extract_fit_engine() %&gt;% rpart.plot()\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n# Model Specification\nrf_spec &lt;- rand_forest() %&gt;%\n  set_engine(engine = 'ranger') %&gt;% \n  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))\n           trees = 1000, # Number of trees\n           min_n = 2,\n           probability = FALSE, # FALSE: get hard predictions (not needed for regression)\n           importance = 'impurity') %&gt;% # we'll come back to this at the end\n  set_mode('classification') # change this for regression\n# Recipe\ndata_rec_rf &lt;- recipe(CO2cat ~ ., data = esgdataAll_sub) %&gt;%\n  update_role(`CountryCode`,new_role = 'ID') %&gt;%\n    update_role(`CountryName`,new_role = 'ID') %&gt;%\n    step_rm(co2) %&gt;%\n    step_nzv(all_predictors()) %&gt;% # removes variables with the same value\n    step_dummy(all_nominal_predictors())  # creates indicator variables for categorical variables\n# Workflows\ndata_wf_mtry2 &lt;- workflow() %&gt;%\n  add_model(rf_spec %&gt;% set_args(mtry = 2)) %&gt;%\n  add_recipe(data_rec_rf)\n## Create workflows for mtry = 4, 9, and 17\ndata_wf_mtry4 &lt;- workflow() %&gt;%\n  add_model(rf_spec %&gt;% set_args(mtry = 4)) %&gt;%\n  add_recipe(data_rec_rf)\ndata_wf_mtry9 &lt;- workflow() %&gt;%\n  add_model(rf_spec %&gt;% set_args(mtry = 9)) %&gt;%\n  add_recipe(data_rec_rf)\ndata_wf_mtry17 &lt;- workflow() %&gt;%\n  add_model(rf_spec %&gt;% set_args(mtry = 17)) %&gt;%\n  add_recipe(data_rec_rf)\n\n\nset.seed(726) # make sure to run this before each fit so that you have the same 1000 trees\ndata_fit_mtry2 &lt;- fit(data_wf_mtry2, data = esgdataAll_sub)\nset.seed(726)\ndata_fit_mtry4 &lt;- fit(data_wf_mtry4, data = esgdataAll_sub)\nset.seed(726) \ndata_fit_mtry9 &lt;- fit(data_wf_mtry9, data = esgdataAll_sub)\nset.seed(726)\ndata_fit_mtry17 &lt;- fit(data_wf_mtry17, data = esgdataAll_sub)\n\n\nrf_OOB_output &lt;- function(fit_model, model_label, truth){\n    tibble(\n          .pred_class = fit_model %&gt;% extract_fit_engine() %&gt;% pluck('predictions'), #OOB predictions\n          class = truth,\n          model = model_label\n      )\n}\n#check out the function output\nrf_OOB_output(data_fit_mtry2,'mtry2', esgdataAll_sub %&gt;% pull(CO2cat))\n\n# A tibble: 713 × 3\n   .pred_class class   model\n   &lt;fct&gt;       &lt;fct&gt;   &lt;chr&gt;\n 1 low_co2     low_co2 mtry2\n 2 low_co2     low_co2 mtry2\n 3 low_co2     low_co2 mtry2\n 4 low_co2     low_co2 mtry2\n 5 low_co2     low_co2 mtry2\n 6 low_co2     low_co2 mtry2\n 7 low_co2     low_co2 mtry2\n 8 low_co2     low_co2 mtry2\n 9 low_co2     low_co2 mtry2\n10 low_co2     low_co2 mtry2\n# ℹ 703 more rows\n\n\n\ndata_rf_OOB_output &lt;- bind_rows(\n    rf_OOB_output(data_fit_mtry2,'mtry2', esgdataAll_sub %&gt;% pull(CO2cat)),\n    rf_OOB_output(data_fit_mtry4,'mtry4', esgdataAll_sub %&gt;% pull(CO2cat)),\n    rf_OOB_output(data_fit_mtry9,'mtry9', esgdataAll_sub %&gt;% pull(CO2cat)),\n    rf_OOB_output(data_fit_mtry17,'mtry17', esgdataAll_sub %&gt;% pull(CO2cat))\n)\ndata_rf_OOB_output %&gt;% \n    group_by(model) %&gt;%\n    accuracy(truth = class, estimate = .pred_class)\n\n# A tibble: 4 × 4\n  model  .metric  .estimator .estimate\n  &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 mtry17 accuracy binary         0.954\n2 mtry2  accuracy binary         0.959\n3 mtry4  accuracy binary         0.959\n4 mtry9  accuracy binary         0.962\n\n\n\ndata_rf_OOB_output %&gt;% \n    group_by(model) %&gt;%\n    accuracy(truth = class, estimate = .pred_class) %&gt;%\n  mutate(mtry = as.numeric(stringr::str_replace(model,'mtry',''))) %&gt;%\n  ggplot(aes(x = mtry, y = .estimate )) + \n  geom_point() +\n  geom_line() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nrf_OOB_output(data_fit_mtry9,'mtry9', esgdataAll_sub %&gt;% pull(CO2cat)) %&gt;%\n    conf_mat(truth = class, estimate= .pred_class)\n\n          Truth\nPrediction low_co2 high_co2\n  low_co2      389       14\n  high_co2      13      297\n\n\n\ndata_fit_mtry9 %&gt;% \n    extract_fit_engine() %&gt;% \n    vip(num_features = 30) + theme_classic() #based on impurity"
  },
  {
    "objectID": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#clustering",
    "href": "Rmd/Final_Code_Appendix_Ayaa_Julia_Ting.html#clustering",
    "title": "Ayaa Asoba, Julia Coelho, Ting Huang stat253_project",
    "section": "Clustering",
    "text": "Clustering\n\nSUB = esgdataAll_sub %&gt;%\n  select(co2,govtEfficacy,agroValue,research,renewableEnergy)\nset.seed(123)\nkclust_k4 &lt;- kmeans(scale(SUB), centers = 4)\nesgdataAll_sub &lt;- esgdataAll_sub %&gt;%\n    mutate(kclust_4 = factor(kclust_k4$cluster))\nggplot(esgdataAll_sub,aes(co2,govtEfficacy,color=kclust_4)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(esgdataAll_sub,aes(co2,agroValue,color=kclust_4)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(esgdataAll_sub,aes(co2,renewableEnergy,color=kclust_4)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(esgdataAll_sub,aes(co2,research,color=kclust_4)) +\n  geom_point()\n\n\n\n\n\n\n\nesgdataAll_sub %&gt;%\n    group_by(kclust_4) %&gt;%\n    summarize(across(c(co2,govtEfficacy,agroValue,research,renewableEnergy), mean))\n\n# A tibble: 4 × 6\n  kclust_4   co2 govtEfficacy agroValue research renewableEnergy\n  &lt;fct&gt;    &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;\n1 1        10.5         1.71       1.64    2.46             22.4\n2 2         1.03       -0.689     20.3     0.193            51.3\n3 3         3.42       -0.107      7.82    0.441            20.6\n4 4         7.85        0.823      2.82    1.01             10.3\n\nesgdataAll_sub %&gt;%\n  count(kclust_4, CountryName)\n\n# A tibble: 102 × 3\n   kclust_4 CountryName        n\n   &lt;fct&gt;    &lt;chr&gt;          &lt;int&gt;\n 1 1        Australia          2\n 2 1        Austria           14\n 3 1        Belgium           14\n 4 1        Canada             9\n 5 1        Czech Republic     2\n 6 1        Denmark           12\n 7 1        Estonia            6\n 8 1        Finland           14\n 9 1        France            13\n10 1        Germany           16\n# ℹ 92 more rows\n\n\n\nesg_cluster_ss &lt;- function(k){\n    # Perform clustering\n    kclust &lt;- kmeans(scale(SUB), centers = k)\n    # Return the total within-cluster sum of squares\n    return(kclust$tot.withinss)\n}\ntibble(\n    k = 1:15,\n    tot_wc_ss = purrr::map_dbl(1:15, esg_cluster_ss)\n) %&gt;% \n    ggplot(aes(x = k, y = tot_wc_ss)) +\n    geom_point() + \n    labs(x = \"Number of clusters\",y = 'Total within-cluster sum of squares') + \n    theme_classic()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPredicting Bike Share Demand in Seoul, South Korea\n\n\n\nMachine Learning\n\nPython\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterference in Causal Inference\n\n\n\nCausal Inference\n\nMethods\n\nWriting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGWAS and Personalities\n\n\n\nResearch\n\nGenetics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnergy and Pearson’s Distances as Metrics for Brain Region Activation Detection in Audiovisual fMRI Study\n\n\n\nSummer Institute in Biostatistics\n\nR\n\nResearch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCO2 Levels in 20 Major Countries\n\n\n\nMachine Learning\n\nR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Turbofan Jet Engines using Survival Analysis Methods\n\n\n\nSurvival Analysis\n\nR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmerica Going Green: A Look at Renewable Energy Generation in the U.S.\n\n\n\nCorrelated Data\n\nR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ting-Chien Huang",
    "section": "",
    "text": "Pronunciation  he/him/his\n\n\nMS in Statistics  2025 | University of Minnesota\nBA in Statistics and Music  2023 | Macalester College"
  },
  {
    "objectID": "index.html#ting-chien-huang",
    "href": "index.html#ting-chien-huang",
    "title": "Ting-Chien Huang",
    "section": "",
    "text": "Pronunciation  he/him/his\n\n\nMS in Statistics  2025 | University of Minnesota\nBA in Statistics and Music  2023 | Macalester College"
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Other",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSenior Capstone-Piano Recital\n\n\n\nMusic\n\n\nPiano\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRacialization of Martial Arts in Bruce Lee Movies\n\n\n\nAsian American Studies\n\n\nMedia and Cultural Studies\n\n\nWriting\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "coursework/Data Prep and EDA/EDA.html",
    "href": "coursework/Data Prep and EDA/EDA.html",
    "title": "Data Prep and Exploratory Data Analysis-Maven Analytics",
    "section": "",
    "text": "Online course through Maven Analytics. Learned about how to approach data science projects through scoping the project to presenting results. Reviewed how to clean, sort, and filter data in Pandas using Jupyter Notebooks. Explored concepts and practical examples of feature engineering. Created visualizations including scatter plots, histograms, and bar charts for EDA using Seaborn. Course included two projects that reinforced the entire data prep and EDA process from raw data to visualizations to data ready for modeling.\nGithub: https://github.com/tchuang1290/Maven-Analytics-Data-Prep-EDA"
  },
  {
    "objectID": "posts/TidyTuesday25-12-09.html",
    "href": "posts/TidyTuesday25-12-09.html",
    "title": "Types of Car Models Produced by Country",
    "section": "",
    "text": "Load Data\n\ntuesdata &lt;- tidytuesdayR::tt_load('2025-12-09')\n\n\nqatarcars &lt;- tuesdata$qatarcars\n\n\n\nLoad Libraries\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(forcats)\n\n\nqatar_cars &lt;- qatarcars %&gt;% \n  group_by(origin) %&gt;% \n  mutate(n_cars = n())\n\n\nqatar_cars %&gt;% \n  ggplot() + \n  geom_bar(aes(y = reorder(origin, n_cars), fill = enginetype), position = 'dodge') +\n  scale_fill_viridis_d(option = \"turbo\") +\n  labs(title = \"Car Models Produced by Country\", x = \"\", y = \"\", fill = \"Engine Type\") +\n  theme_minimal() + \n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\nThis plot displays the types of car models, by engine type, produced by several different countries. It is important to note that this is a toy dataset, so any conclusions drawn from this plot may not reflect reality. In this dataset, Japan produces the most models of cars in total and also the most models of petrol and hybrid cars. They do not produce any electric car models. The U.S. produces the most electric car models but produces a relatively small amount of car models overall. Sweden produces the least amount of car models overall and only produces hybrid cars. Italy only produces petrol car models. Only three countries produce electric, hybrid, and petrol car models: Germany, the UK, and South Korea. Most countries (other than Sweden) produce more petrol car models than any other engine type.\nData obtained from: Data Science Learning Community (2024). Tidy Tuesday: A weekly social data project. https://tidytues.day"
  },
  {
    "objectID": "posts/tidytuesday26-1-12.html",
    "href": "posts/tidytuesday26-1-12.html",
    "title": "Tidy Tuesday 2026-01-12",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.3.3\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(rnaturalearth)"
  },
  {
    "objectID": "posts/tidytuesday26-1-12.html#load-packages",
    "href": "posts/tidytuesday26-1-12.html#load-packages",
    "title": "Tidy Tuesday 2026-01-12",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(sf)\n\nWarning: package 'sf' was built under R version 4.3.3\n\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(rnaturalearth)"
  },
  {
    "objectID": "posts/tidytuesday26-1-12.html#load-data",
    "href": "posts/tidytuesday26-1-12.html#load-data",
    "title": "Tidy Tuesday 2026-01-12",
    "section": "Load Data",
    "text": "Load Data\n\ntuesdata &lt;- tidytuesdayR::tt_load(2026, week = 2)\n\n---- Compiling #TidyTuesday Information for 2026-01-13 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"africa.csv\"\n\nafrica &lt;- tuesdata$africa"
  },
  {
    "objectID": "posts/tidytuesday26-1-12.html#data-exploration",
    "href": "posts/tidytuesday26-1-12.html#data-exploration",
    "title": "Tidy Tuesday 2026-01-12",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nwhich(is.na(africa))\n\ninteger(0)\n\n\n\nafrica %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# A tibble: 68 × 4\n   language family      native_speakers country \n   &lt;chr&gt;    &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;   \n 1 Afar     Afroasiatic         2500000 Ethiopia\n 2 Afar     Afroasiatic         2500000 Djibouti\n 3 Afar     Afroasiatic         2500000 Eritrea \n 4 Afar     Afroasiatic         2500000 Djibouti\n 5 Afar     Afroasiatic         2500000 Eritrea \n 6 Afar     Afroasiatic         2500000 Ethiopia\n 7 Bariba   Niger–Congo         1100000 Benin   \n 8 Bariba   Niger–Congo         1100000 Benin   \n 9 Bangi    Niger–Congo          120000 Congo   \n10 Bangi    Niger–Congo          120000 Congo   \n# ℹ 58 more rows\n\n\n\nafrica_clean &lt;- unique(africa)\n\n\nafrica_clean %&gt;% \n  group_by_all() %&gt;% \n  filter(n() &gt; 1) %&gt;% \n  ungroup()\n\n# A tibble: 0 × 4\n# ℹ 4 variables: language &lt;chr&gt;, family &lt;chr&gt;, native_speakers &lt;dbl&gt;,\n#   country &lt;chr&gt;\n\n\n\nafrica_lang &lt;- africa_clean %&gt;% \n  group_by(country) %&gt;% \n  tally(name = \"num_langs_spoken\")\n\n\nafrica_lang$country &lt;- tolower(africa_lang$country)\n\n\nafrica_lang[rep(12,2),]\n\n# A tibble: 2 × 2\n  country num_langs_spoken\n  &lt;chr&gt;              &lt;int&gt;\n1 congo                 80\n2 congo                 80\n\n\n\nafrica_lang1 &lt;- rbind(africa_lang[-12,],africa_lang[rep(12,2),])\n\n\nafrica_lang1[51, 1] = \"democratic republic of the congo\"\nafrica_lang1[52, 1] = \"republic of the congo\"\n\n\nafrica_countries &lt;- ne_countries(\n  continent = \"Africa\",\n  scale = \"medium\",\n  returnclass = \"sf\"\n)\n\n\nafrica_countries$geounit &lt;- tolower(africa_countries$geounit)\n\n\nafrica_countries1 &lt;- africa_countries %&gt;% \n  select(geounit, geometry)\n\n\nafrica_countries2 &lt;- africa_countries1 %&gt;% \n  left_join(africa_lang1, by = c(\"geounit\" = \"country\"))\n\n\nggplot(africa_countries2) +\n  geom_sf(aes(fill = num_langs_spoken), color = \"white\", linewidth = 0.2) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(\n    title = \"Number of Languages Spoken in African Countries\",\n    caption = \"Source: Natural Earth and Data Science Learning Community\",\n    fill = \"\"\n  ) +\n  theme_void()"
  }
]